{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9egJDg54BFAn",
        "outputId": "7d7420e2-8113-453f-fd39-3842d4ae42d4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.237-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cu126)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.3.237-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.237 ultralytics-thop-2.0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dilTdO4KA4xc",
        "outputId": "ff0c9d98-a453-4299-b722-189d2739e88e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import datetime\n",
        "import sys\n",
        "from filelock import FileLock\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import yaml\n",
        "import torchvision\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c83VM3l7A4xd"
      },
      "source": [
        "════════════════════════════════════════<br>\n",
        "▣ 01. 디렉토리 및 유틸 함수 설정<br>\n",
        "════════════════════════════════════════"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0naGF9TaCaEe",
        "outputId": "e52bafc1-fca5-43d1-fd52-b354dfe4e851"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEH1ukuCA4xe",
        "outputId": "29eaeb16-f61a-43d5-9c55-b9f7a24eb6d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "설정된 BASE_DIR: /content/drive/MyDrive/oraldrug/99.drive.zip\n"
          ]
        }
      ],
      "source": [
        "VER = \"2025.12.12.001.bestOne\"\n",
        "#BASE_DIR = \"/content/drive/MyDrive/codeit/data\"\n",
        "#BASE_DIR = r\"D:\\01.project\\EntryPrj\\data\"\n",
        "\n",
        "ZIP_DIR = '/content/drive/MyDrive/oraldrug/99.drive.zip'\n",
        "print(\"설정된 BASE_DIR:\", ZIP_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"압축 파일 복사 시작...\")\n",
        "!cp \"$ZIP_DIR\" /content/\n",
        "print(\"복사 완료.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIpEGRWWlvif",
        "outputId": "eab8ae98-ecde-40c9-b49f-25505d8724c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축 파일 복사 시작...\n",
            "복사 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '/content/images' # 출력 파일(로그, 모델 등)이 저장될 실제 디렉토리 경로\n",
        "print(\"압축 풀기 시작...\")\n",
        "!unzip -q /content/99.drive.zip -d \"$BASE_DIR\"\n",
        "print(f\"압축 풀기 완료. 이제 {BASE_DIR} 폴더를 사용해 학습을 시작하세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEgW2fPYmM9J",
        "outputId": "ea6711e0-4086-4585-8f84-3b413768c84e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "압축 풀기 시작...\n",
            "replace /content/images/test_images/1.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/images/test_images/10.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "압축 풀기 완료. 이제 /content/images 폴더를 사용해 학습을 시작하세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qg_UYFPlA4xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4690b5cf-ef43-4bff-8fbe-90a25b0a687c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "설정된 LOG_FILE: /content/images/operation.log\n",
            "설정된 SUBMISSTION_DIR: /content/images/submission\n"
          ]
        }
      ],
      "source": [
        "LOG_FILE = os.path.join(BASE_DIR, \"operation.log\")\n",
        "SUBMISSTION_DIR = os.path.join(BASE_DIR, \"submission\")\n",
        "\n",
        "print(f\"설정된 LOG_FILE: {LOG_FILE}\")\n",
        "print(f\"설정된 SUBMISSTION_DIR: {SUBMISSTION_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nMtPLOpA4xf"
      },
      "source": [
        "데이터 디렉토리 설정 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBpuTUAHA4xf",
        "outputId": "7d135424-c6d9-4caa-b639-d6c6e44209a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:6: SyntaxWarning: invalid escape sequence '\\i'\n",
            "<>:6: SyntaxWarning: invalid escape sequence '\\i'\n",
            "/tmp/ipython-input-2085361606.py:6: SyntaxWarning: invalid escape sequence '\\i'\n",
            "  data_dir: 데이터 루트 디렉토리 (예: content\\images\\)\n"
          ]
        }
      ],
      "source": [
        "def GetConfig(data_dir):\n",
        "    \"\"\"\n",
        "    데이터 디렉토리로부터 필요한 경로들을 생성\n",
        "\n",
        "    Args:\n",
        "        data_dir: 데이터 루트 디렉토리 (예: content\\images\\)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (image_dir, annotation_dir, yaml_file, yaml_label_dir, test_img_dir)\n",
        "    \"\"\"\n",
        "    image_dir = os.path.join(data_dir, \"train_images\")\n",
        "    annotation_dir = os.path.join(data_dir, \"train_annotations\")\n",
        "    yaml_file = os.path.join(data_dir, \"yolo_yaml.yaml\")\n",
        "    yaml_label_dir = os.path.join(data_dir, \"yolo_labels\")\n",
        "    # test_img_dir는 data_dir 내부에 위치하는 것으로 수정\n",
        "    test_img_dir = os.path.join(data_dir, \"test_images\")\n",
        "\n",
        "    print(f\"설정된 image_dir: {image_dir}\")\n",
        "    print(f\"설정된 test_img_dir: {test_img_dir}\")\n",
        "\n",
        "    return image_dir, annotation_dir, yaml_file, yaml_label_dir, test_img_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCNZEW_dA4xf"
      },
      "source": [
        "공통 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2eY9H7UBA4xf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e510b524-b14a-4ee5-ce01-26ee8736a94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "설정된 MODEL_FILES: /content/images/modelfiles\n",
            "설정된 RESULT_CSV: /content/images/entryprj.csv\n"
          ]
        }
      ],
      "source": [
        "MODEL_FILES = os.path.join(BASE_DIR, \"modelfiles\")\n",
        "RESULT_CSV = os.path.join(BASE_DIR, \"entryprj.csv\")\n",
        "DEVICE_TYPE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"설정된 MODEL_FILES: {MODEL_FILES}\")\n",
        "print(f\"설정된 RESULT_CSV: {RESULT_CSV}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7vlI7kYA4xg"
      },
      "source": [
        " 구분선 출력 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rEQr5xgeA4xg"
      },
      "outputs": [],
      "source": [
        "def Lines(text=\"\", count=100):\n",
        "    print(\"═\" * count)\n",
        "    if text != \"\":\n",
        "        print(f\"{text}\")\n",
        "        print(\"═\" * count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C305XqsKA4xg"
      },
      "source": [
        " 현재 시간 문자열 반환 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_9L8NgxAA4xg"
      },
      "outputs": [],
      "source": [
        "def now_str():\n",
        "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS2U9teQA4xg"
      },
      "source": [
        " 디렉토리 생성 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "imCxyUZaA4xg"
      },
      "outputs": [],
      "source": [
        "def makedirs(d):\n",
        "    os.makedirs(d, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlbGDb7bA4xh"
      },
      "source": [
        " 운영 로그 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "59gUwmJbA4xh"
      },
      "outputs": [],
      "source": [
        "def OpLog(log, bLines=True):\n",
        "    if bLines:\n",
        "        Lines(f\"[{now_str()}] {log}\")\n",
        "    try:\n",
        "        caller_name = sys._getframe(1).f_code.co_name\n",
        "    except Exception:\n",
        "        caller_name = \"UnknownFunction\"\n",
        "    log_filename = LOG_FILE\n",
        "    log_lock_filename = log_filename + \".lock\"\n",
        "    log_content = f\"[{now_str()}] {caller_name}: {log}\\n\"\n",
        "    try:\n",
        "        lock = FileLock(log_lock_filename, timeout=10)\n",
        "        with lock:\n",
        "            with open(log_filename, \"a\", encoding=\"utf-8\") as f:\n",
        "                f.write(log_content)\n",
        "    except Exception as e:\n",
        "        print(f\"Log write error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwhczpweA4xh",
        "outputId": "4315dd67-bc57-4ddc-e39c-49f9c42715eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:09:52] Start program.2025.12.12.001.bestOne\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        }
      ],
      "source": [
        "OpLog(f\"Start program.{VER}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6MG-V7nA4xh"
      },
      "source": [
        "════════════════════════════════════════<br>\n",
        "▣ 02. 클래스 수 계산 및 클래스 매핑 생성<br>\n",
        "════════════════════════════════════════<br>\n",
        "train_annotations에서 JSON 파일을 읽어서 category_id로 클래스 수 계산"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "BQJNKmMmA4xh"
      },
      "outputs": [],
      "source": [
        "def count_classes(annotations_dir):\n",
        "    \"\"\"\n",
        "    모든 서브 디렉토리에서 JSON을 읽어서 category_id를 classes로 카운트\n",
        "    os.walk()를 사용하여 모든 하위 디렉토리를 재귀적으로 검사\n",
        "    Returns:\n",
        "        int: 고유 category_id 개수\n",
        "    \"\"\"\n",
        "    unique_category_ids = set()\n",
        "\n",
        "    # os.walk()로 모든 하위 디렉토리 재귀적으로 탐색\n",
        "    for root, dirs, files in os.walk(annotations_dir):\n",
        "        for json_file in files:\n",
        "            if json_file.endswith(\".json\"):\n",
        "                json_path = os.path.join(root, json_file)\n",
        "                try:\n",
        "                    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                        data = json.load(f)\n",
        "                    # category_id 추출\n",
        "                    if \"annotations\" in data and len(data[\"annotations\"]) > 0:\n",
        "                        for ann in data[\"annotations\"]:\n",
        "                            category_id = ann.get(\"category_id\")\n",
        "                            if category_id is not None:\n",
        "                                # category_id를 int로 변환\n",
        "                                try:\n",
        "                                    category_id = int(category_id)\n",
        "                                    unique_category_ids.add(category_id)\n",
        "                                except (ValueError, TypeError):\n",
        "                                    continue\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "    return len(unique_category_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3HPQE1GUA4xh"
      },
      "outputs": [],
      "source": [
        "def get_class_mapping(annotations_dir):\n",
        "    \"\"\"\n",
        "    annotation 디렉토리에서 JSON을 읽어 category_id 기반으로 클래스 매핑 정보를 반환\n",
        "    os.walk()를 사용하여 모든 하위 디렉토리를 재귀적으로 검사\n",
        "    Args:\n",
        "        annotations_dir: annotation 디렉토리 경로\n",
        "    Returns:\n",
        "        tuple: (class_dirs, class_to_idx, idx_to_class, unique_classes)\n",
        "            - class_dirs: [(category_id, [json_paths]), ...] 리스트\n",
        "            - class_to_idx: {category_id: index} 딕셔너리 (category_id를 그대로 사용)\n",
        "            - idx_to_class: {index: category_id} 딕셔너리\n",
        "            - unique_classes: 정렬된 고유 category_id 리스트\n",
        "    \"\"\"\n",
        "    class_info = {}  # {category_id: [json_path1, json_path2, ...]}\n",
        "    class_dirs = []\n",
        "\n",
        "    # os.walk()로 모든 하위 디렉토리 재귀적으로 탐색\n",
        "    json_count = 0\n",
        "    for root, dirs, files in os.walk(annotations_dir):\n",
        "        for json_file in files:\n",
        "            if json_file.endswith(\".json\"):\n",
        "                json_path = os.path.join(root, json_file)\n",
        "                json_count += 1\n",
        "                try:\n",
        "                    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                        data = json.load(f)\n",
        "                    if \"annotations\" in data and len(data[\"annotations\"]) > 0:\n",
        "                        for ann in data[\"annotations\"]:\n",
        "                            category_id = ann.get(\"category_id\")\n",
        "                            if category_id is not None:\n",
        "                                # category_id를 int로 변환 (문자열인 경우 대비)\n",
        "                                try:\n",
        "                                    category_id = int(category_id)\n",
        "                                except (ValueError, TypeError):\n",
        "                                    continue\n",
        "\n",
        "                                # 동일 category_id가 여러 JSON 파일에 있을 수 있으므로 모두 저장\n",
        "                                if category_id not in class_info:\n",
        "                                    class_info[category_id] = []\n",
        "                                if json_path not in class_info[category_id]:\n",
        "                                    class_info[category_id].append(json_path)\n",
        "                                break  # 한 JSON에서 category_id 하나만 찾으면 됨\n",
        "                except Exception as e:\n",
        "                    OpLog(f\"Error reading {json_path}: {e}\", bLines=False)\n",
        "                    continue\n",
        "    OpLog(f\"get_class_mapping: 총 {json_count}개 JSON 파일 스캔, {len(class_info)}개 클래스 발견\", bLines=False)\n",
        "\n",
        "    # class_dirs 생성 (category_id, JSON 파일 목록)\n",
        "    for category_id, json_paths in class_info.items():\n",
        "        class_dirs.append((category_id, json_paths))\n",
        "\n",
        "    # 클래스 정렬 및 인덱스 매핑 (0-based index로 변환)\n",
        "    unique_classes = sorted(class_info.keys())\n",
        "    # category_id를 0부터 시작하는 인덱스로 매핑\n",
        "    class_to_idx = {category_id: idx for idx, category_id in enumerate(unique_classes)}\n",
        "    idx_to_class = {idx: category_id for idx, category_id in enumerate(unique_classes)}\n",
        "    return class_dirs, class_to_idx, idx_to_class, unique_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HW3cI-WNA4xi"
      },
      "outputs": [],
      "source": [
        "def analyze_image_json_mapping(train_img_dir, annotation_dir, output_dir):\n",
        "    \"\"\"\n",
        "    이미지와 JSON 파일 간의 매핑 관계를 분석하여 CSV 파일로 저장\n",
        "\n",
        "    Args:\n",
        "        train_img_dir: 학습 이미지 디렉토리\n",
        "        annotation_dir: 어노테이션 디렉토리\n",
        "        output_dir: CSV 파일 저장 디렉토리\n",
        "    생성 파일:\n",
        "        - img_to_json.csv: 이미지 -> JSON 매핑 (이미지명, JSON경로)\n",
        "        - json_to_img.csv: JSON -> 이미지 매핑 (JSON경로, 이미지명)\n",
        "    \"\"\"\n",
        "    OpLog(\"이미지-JSON 매핑 분석 시작\", bLines=True)\n",
        "\n",
        "    img_to_json_file = os.path.join(output_dir, \"img_to_json.csv\")\n",
        "    json_to_img_file = os.path.join(output_dir, \"json_to_img.csv\")\n",
        "\n",
        "    # 1. train_img_dir 밑의 모든 이미지 파일 수집\n",
        "    image_files = {}  # {filename: full_path}\n",
        "    if os.path.exists(train_img_dir):\n",
        "        for img_file in os.listdir(train_img_dir):\n",
        "            if img_file.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\")):\n",
        "                image_files[img_file] = os.path.join(train_img_dir, img_file)\n",
        "    OpLog(f\"이미지 파일 수: {len(image_files)}\", bLines=False)\n",
        "\n",
        "    # 2. annotation_dir 밑의 모든 JSON 파일 수집\n",
        "    json_files = []  # [(json_path, images_in_json), ...]\n",
        "    if os.path.exists(annotation_dir):\n",
        "        for subdir in os.listdir(annotation_dir):\n",
        "            subdir_path = os.path.join(annotation_dir, subdir)\n",
        "            if os.path.isdir(subdir_path):\n",
        "                for class_dir in os.listdir(subdir_path):\n",
        "                    class_dir_path = os.path.join(subdir_path, class_dir)\n",
        "                    if os.path.isdir(class_dir_path):\n",
        "                        for json_file in os.listdir(class_dir_path):\n",
        "                            if json_file.endswith(\".json\"):\n",
        "                                json_path = os.path.join(class_dir_path, json_file)\n",
        "                                # JSON 파일에서 이미지 정보 추출\n",
        "                                try:\n",
        "                                    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                                        data = json.load(f)\n",
        "                                    images_in_json = []\n",
        "                                    if \"images\" in data:\n",
        "                                        for img_info in data[\"images\"]:\n",
        "                                            img_filename = img_info.get(\"file_name\", \"\")\n",
        "                                            if img_filename:\n",
        "                                                images_in_json.append(img_filename)\n",
        "                                    json_files.append((json_path, images_in_json))\n",
        "                                except Exception as e:\n",
        "                                    OpLog(\n",
        "                                        f\"JSON 파일 읽기 오류 {json_path}: {e}\",\n",
        "                                        bLines=False,\n",
        "                                    )\n",
        "    OpLog(f\"JSON 파일 수: {len(json_files)}\", bLines=False)\n",
        "\n",
        "    # 3. IMG_TO_JSON 매핑 생성\n",
        "    img_to_json_mapping = []  # [(img_name, json_path), ...]\n",
        "    for img_name in sorted(image_files.keys()):\n",
        "        found_jsons = []\n",
        "\n",
        "        # 이 이미지를 포함하는 JSON 파일 찾기\n",
        "        for json_path, images_in_json in json_files:\n",
        "            if img_name in images_in_json:\n",
        "                found_jsons.append(json_path)\n",
        "        if found_jsons:\n",
        "            for json_path in found_jsons:\n",
        "                img_to_json_mapping.append((img_name, json_path))\n",
        "        else:\n",
        "            # JSON 파일이 없는 이미지\n",
        "            img_to_json_mapping.append((img_name, \"NONE\"))\n",
        "\n",
        "    # 4. JSON_TO_IMG 매핑 생성\n",
        "    json_to_img_mapping = []  # [(json_path, img_name), ...]\n",
        "    for json_path, images_in_json in json_files:\n",
        "        if images_in_json:\n",
        "            for img_name in images_in_json:\n",
        "                # 실제 이미지 파일이 존재하는지 확인\n",
        "                if img_name in image_files:\n",
        "                    json_to_img_mapping.append((json_path, img_name))\n",
        "                else:\n",
        "                    json_to_img_mapping.append((json_path, \"NONE\"))\n",
        "        else:\n",
        "            # 이미지 정보가 없는 JSON\n",
        "            json_to_img_mapping.append((json_path, \"NONE\"))\n",
        "\n",
        "    # 5. IMG_TO_JSON CSV 파일 저장\n",
        "    makedirs(os.path.dirname(img_to_json_file))\n",
        "    with open(img_to_json_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"Image,JSON\\n\")\n",
        "        for img_name, json_path in img_to_json_mapping:\n",
        "            f.write(f\"{img_name},{json_path}\\n\")\n",
        "    OpLog(\n",
        "        f\"IMG_TO_JSON 저장 완료: {img_to_json_file} ({len(img_to_json_mapping)}개 매핑)\",\n",
        "        bLines=False,\n",
        "    )\n",
        "\n",
        "    # 6. JSON_TO_IMG CSV 파일 저장\n",
        "    with open(json_to_img_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"JSON,Image\\n\")\n",
        "        for json_path, img_name in json_to_img_mapping:\n",
        "            f.write(f\"{json_path},{img_name}\\n\")\n",
        "    OpLog(\n",
        "        f\"JSON_TO_IMG 저장 완룜: {json_to_img_file} ({len(json_to_img_mapping)}개 매핑)\",\n",
        "        bLines=False,\n",
        "    )\n",
        "\n",
        "    # 7. 통계 정보 출력\n",
        "    img_without_json = sum(\n",
        "        1 for _, json_path in img_to_json_mapping if json_path == \"NONE\"\n",
        "    )\n",
        "    json_without_img = sum(\n",
        "        1 for _, img_name in json_to_img_mapping if img_name == \"NONE\"\n",
        "    )\n",
        "    OpLog(f\"매핑 분석 완료:\", bLines=True)\n",
        "    OpLog(f\"  - 전체 이미지: {len(image_files)}개\", bLines=False)\n",
        "    OpLog(f\"  - 전체 JSON: {len(json_files)}개\", bLines=False)\n",
        "    OpLog(f\"  - JSON 없는 이미지: {img_without_json}개\", bLines=False)\n",
        "    OpLog(f\"  - 이미지 없는 JSON: {json_without_img}개\", bLines=False)\n",
        "    return img_to_json_mapping, json_to_img_mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll06uCQ5A4xi"
      },
      "source": [
        "════════════════════════════════════════<br>\n",
        "▣ 03. 데이터셋 및 데이터 증강 함수 정의<br>\n",
        "════════════════════════════════════════<br>\n",
        "다양한 데이터 증강(transform) 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dE_ciBzOA4xi"
      },
      "outputs": [],
      "source": [
        "def GetTransform(transform_type=\"default\"):\n",
        "    if transform_type == \"default\":\n",
        "        return transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "    if transform_type == \"A\":\n",
        "        return transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ColorJitter(\n",
        "                    brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1\n",
        "                ),\n",
        "                transforms.RandomRotation(degrees=15),\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "    elif transform_type == \"B\":\n",
        "        return transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.RandomAffine(\n",
        "                    degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)\n",
        "                ),\n",
        "                transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        return transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJHr1xfcA4xi"
      },
      "source": [
        "커스텀 데이터셋 클래스 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RcsuES-AA4xi"
      },
      "outputs": [],
      "source": [
        "class PillDataset(Dataset):\n",
        "    def __init__(self, annotations_dir, img_dir, transform=None, is_test=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            annotations_dir: train_annotations 경로 (is_test=True일 경우 무시됨)\n",
        "            img_dir: train_images 또는 test_images 경로\n",
        "            transform: 이미지 변환 함수\n",
        "            is_test: 테스트 데이터셋 여부 (True이면 annotation 없이 이미지만 로드)\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        self.samples = (\n",
        "            []\n",
        "        )  # (img_path, label_idx, class_name) 튜플 리스트 또는 (img_path,) 튜플\n",
        "        self.class_to_idx = {}  # {class_name: idx}\n",
        "        self.idx_to_class = {}  # {idx: class_name}\n",
        "        if is_test:\n",
        "            # 테스트 데이터셋: annotation 없이 이미지만 로드\n",
        "            if not os.path.exists(img_dir):\n",
        "                OpLog(\n",
        "                    f\"테스트 이미지 디렉토리를 찾을 수 없습니다: {img_dir}\", bLines=True\n",
        "                )\n",
        "                return\n",
        "\n",
        "            # 이미지 디렉토리의 모든 이미지 파일 수집\n",
        "            for img_file in os.listdir(img_dir):\n",
        "                if img_file.lower().endswith(\n",
        "                    (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\")\n",
        "                ):\n",
        "                    img_path = os.path.join(img_dir, img_file)\n",
        "                    self.samples.append((img_path,))  # 테스트는 레이블 없음\n",
        "            OpLog(f\"테스트 이미지 {len(self.samples)}개 로드 완료\", bLines=False)\n",
        "        else:\n",
        "            # 학습/검증 데이터셋: annotation 사용\n",
        "            # get_class_mapping 함수 사용하여 클래스 매핑 정보 가져오기\n",
        "            class_dirs, self.class_to_idx, self.idx_to_class, self._unique_classes = (\n",
        "                get_class_mapping(annotations_dir)\n",
        "            )\n",
        "\n",
        "            # 각 클래스의 annotation 파일 읽기\n",
        "            for category_id, json_paths in class_dirs:\n",
        "                label_idx = self.class_to_idx[category_id]\n",
        "                # 해당 category_id를 가진 모든 JSON 파일 읽기\n",
        "                for json_path in json_paths:\n",
        "                    try:\n",
        "                        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                            data = json.load(f)\n",
        "                        # images 정보 추출\n",
        "                        if \"images\" in data:\n",
        "                            for img_info in data[\"images\"]:\n",
        "                                img_filename = img_info[\"file_name\"]\n",
        "                                img_path = os.path.join(self.img_dir, img_filename)\n",
        "                                # 이미지 파일이 실제로 존재하는지 확인\n",
        "                                if os.path.exists(img_path):\n",
        "                                    self.samples.append(\n",
        "                                        (img_path, label_idx, category_id)\n",
        "                                    )\n",
        "                    except Exception as e:\n",
        "                        OpLog(f\"Error reading {json_path}: {e}\", bLines=False)\n",
        "            OpLog(f\"PillDataset 로드 완료: {len(self.samples)}개 샘플\", bLines=False)\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_test:\n",
        "            # 테스트 데이터: 이미지만 반환 (레이블 없음)\n",
        "            img_path = self.samples[idx][0]\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            # 이미지와 파일명 반환 (예측 후 결과 매칭용)\n",
        "            return image, os.path.basename(img_path)\n",
        "        else:\n",
        "            # 학습/검증 데이터: 이미지와 레이블 반환\n",
        "            img_path, label, category_id = self.samples[idx]  # category_id는 클래스 ID\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "OkEiZgkPA4xi"
      },
      "outputs": [],
      "source": [
        "def GetDataset(annotations_dir, img_dir, transform_type=\"default\", is_test=False):\n",
        "    \"\"\"\n",
        "    데이터셋 생성\n",
        "    Args:\n",
        "        annotations_dir: annotation 디렉토리 경로 (is_test=True일 경우 무시됨)\n",
        "        img_dir: 이미지 디렉토리 경로\n",
        "        transform_type: 변환 타입 ('default', 'A', 'B')\n",
        "        is_test: 테스트 데이터셋 여부\n",
        "    \"\"\"\n",
        "    transform = GetTransform(transform_type)\n",
        "    dataset = PillDataset(annotations_dir, img_dir, transform, is_test=is_test)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "zUrFE4OJA4xi"
      },
      "outputs": [],
      "source": [
        "def GetLoaders(\n",
        "    annotations_dir,\n",
        "    transform_type,\n",
        "    img_dir,\n",
        "    test_img_dir,\n",
        "    batch_size=32,\n",
        "    train_ratio=0.8,\n",
        "    num_workers=4,\n",
        "):\n",
        "    \"\"\"\n",
        "    전체 데이터셋을 train/val로 분할하여 DataLoader 생성\n",
        "\n",
        "    Args:\n",
        "        annotations_dir: 어노테이션 디렉토리\n",
        "        transform_type: 변환 타입\n",
        "        img_dir: 학습 이미지 디렉토리\n",
        "        test_img_dir: 테스트 이미지 디렉토리\n",
        "        batch_size: 배치 크기\n",
        "        train_ratio: 학습 데이터 비율\n",
        "        num_workers: 워커 수\n",
        "    \"\"\"\n",
        "    from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "    # 전체 데이터셋 로드 (train용 augmentation)\n",
        "    full_dataset = GetDataset(annotations_dir, img_dir, transform_type=transform_type)\n",
        "    # Train/Val 분할\n",
        "    total_size = len(full_dataset)\n",
        "    train_size = int(total_size * train_ratio)\n",
        "    val_size = total_size - train_size\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "    # Validation 데이터셋에는 augmentation 없이 기본 transform만 적용\n",
        "    val_dataset_plain = GetDataset(\n",
        "        annotations_dir, img_dir, transform_type=transform_type\n",
        "    )\n",
        "    val_indices = val_dataset.indices\n",
        "    val_dataset = torch.utils.data.Subset(val_dataset_plain, val_indices)\n",
        "    # DataLoader 생성\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
        "    )\n",
        "    test_loader = GetTestLoader(test_img_dir, batch_size=batch_size, num_workers=num_workers)\n",
        "    OpLog(\n",
        "        f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\",\n",
        "        bLines=False,\n",
        "    )\n",
        "    OpLog(f\"Total classes: {len(full_dataset.class_to_idx)}\", bLines=False)\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Vk-st6WoA4xj"
      },
      "outputs": [],
      "source": [
        "def GetTestLoader(test_img_dir, batch_size=16, num_workers=4):\n",
        "    \"\"\"\n",
        "    테스트 데이터셋 로더 생성 (annotation 없음)\n",
        "    Args:\n",
        "        test_img_dir: 테스트 이미지 디렉토리\n",
        "        batch_size: 배치 크기\n",
        "        num_workers: 워커 수\n",
        "    Returns:\n",
        "        test_loader: 테스트 데이터 로더\n",
        "    \"\"\"\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    # 테스트 데이터셋 로드 (annotation 없음, 증강 없음)\n",
        "    test_dataset = GetDataset(\n",
        "        annotations_dir=None,  # 테스트는 annotation 불필요\n",
        "        img_dir=test_img_dir,\n",
        "        transform_type=\"default\",  # 증강 없음\n",
        "        is_test=True,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,  # 테스트는 shuffle 안함\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    OpLog(f\"Test samples: {len(test_dataset)}\", bLines=False)\n",
        "    return test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-HiDsyPA4xj"
      },
      "source": [
        "사용 예시:<br>\n",
        "data_dir = r\"D:\\01.project\\EntryPrj\\data\\oraldrug\\1.drug_Image_annotation_allOK\"<br>\n",
        "image_dir, annotation_dir, yaml_file, yaml_label_dir, test_img_dir = GetConfig(data_dir)<br>\n",
        "train_loader, val_loader, test_loader = GetLoaders(<br>\n",
        "    annotation_dir, \"A\", image_dir, test_img_dir, batch_size=16, train_ratio=0.8, num_workers=2<br>\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_vR9WECA4xj"
      },
      "source": [
        "════════════════════════════════════════<br>\n",
        "▣ 04. 기본 모델 클래스 정의<br>\n",
        "════════════════════════════════════════"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RKCsccK-A4xj"
      },
      "outputs": [],
      "source": [
        "class BaseModel(nn.Module):\n",
        "    \"\"\"모델의 기본 클래스 - save/load 등 공통 기능 제공\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__() # Changed from super(BaseModel, self).__init__()\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.test_losses = []\n",
        "        self.best_val_loss = float(\"inf\")\n",
        "    def getMyName(self):\n",
        "        return self.__class__.__name__\n",
        "\n",
        "    ## 모델 저장 함수\n",
        "    def save_model(self, epoch_index, is_best=False, **kwargs):\n",
        "        \"\"\"현재 모델 상태를 저장\n",
        "        Args:\n",
        "            epoch_index: 현재 에포크 번호\n",
        "            is_best: Best 모델인지 여부\n",
        "            **kwargs: 추가로 저장할 데이터 (model_state_dict, train_losses 등)\n",
        "        \"\"\"\n",
        "        save_dir = MODEL_FILES\n",
        "        makedirs(save_dir)\n",
        "        model_name = self.getMyName()\n",
        "\n",
        "        # 서브클래스에서 재정의할 수 있도록 파일명 결정 로직\n",
        "        if is_best:\n",
        "            filename = os.path.join(save_dir, f\"{model_name}_best_model.pth\")\n",
        "        else:\n",
        "            filename = os.path.join(save_dir, f\"{model_name}_epoch_{epoch_index}.pth\")\n",
        "\n",
        "        # 기본 저장 데이터\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch_index,\n",
        "            \"is_best\": is_best,\n",
        "            \"model_name\": model_name,\n",
        "        }\n",
        "\n",
        "        # kwargs로 전달된 추가 데이터 저장\n",
        "        checkpoint.update(kwargs)\n",
        "        torch.save(checkpoint, filename)\n",
        "        if is_best:\n",
        "            print(f\"  Best 모델 저장됨: {filename}\")\n",
        "            OpLog(f\"Best model saved: {filename}\")\n",
        "        else:\n",
        "            OpLog(f\"모델 저장됨: {filename}\", bLines=False)\n",
        "\n",
        "    ## 모델 로드 함수\n",
        "    def load_model(self, model_file, **kwargs):\n",
        "        \"\"\"저장된 모델 상태를 로드\n",
        "        Args:\n",
        "            model_file: 모델 파일 경로\n",
        "            **kwargs: 로드 관련 추가 옵션\n",
        "        Returns:\n",
        "            dict: 체크포인트 데이터 또는 None\n",
        "        \"\"\"\n",
        "        if not os.path.exists(model_file):\n",
        "            OpLog(f\"모델 파일을 찾을 수 없습니다: {model_file}\", bLines=True)\n",
        "            return None\n",
        "        checkpoint = torch.load(model_file, map_location=DEVICE_TYPE, weights_only=False)\n",
        "        OpLog(\n",
        "            f\"모델 로드 완료: {model_file} (Epoch {checkpoint['epoch']})\", bLines=True\n",
        "        )\n",
        "        return checkpoint\n",
        "\n",
        "    ## 학습 이력 저장 함수 (객체 탐지 모델용)\n",
        "    def save_metrics_to_csv(\n",
        "        self,\n",
        "        model_name,\n",
        "        epoch_index,\n",
        "        max_epochs,\n",
        "        train_loss,\n",
        "        current_lr,\n",
        "        val_loss=None,\n",
        "        test_loss=None,\n",
        "        mAP50=None,\n",
        "        mAP50_95=None,\n",
        "        precision=None,\n",
        "        recall=None,\n",
        "        total_detections=None,\n",
        "        avg_confidence=None,\n",
        "        mode=\"train\",\n",
        "    ):\n",
        "        \"\"\"객체 탐지 모델 학습 메트릭을 CSV 파일에 저장\n",
        "        Args:\n",
        "            model_name: 모델 이름 (FasterRCNNModel, YOLOv8Model 등)\n",
        "            epoch_index: 현재 에포크 (1-based)\n",
        "            max_epochs: 최대 에포크\n",
        "            train_loss: 학습 손실\n",
        "            current_lr: 현재 학습률\n",
        "            val_loss: 검증 손실 (optional)\n",
        "            test_loss: 테스트 손실 (optional)\n",
        "            mAP50: mAP@0.5 메트릭 (optional)\n",
        "            mAP50_95: mAP@0.5:0.95 메트릭 (optional)\n",
        "            precision: Precision 메트릭 (optional)\n",
        "            recall: Recall 메트릭 (optional)\n",
        "            total_detections: 총 탐지 개수 (optional)\n",
        "            avg_confidence: 평균 신뢰도 (optional)\n",
        "            mode: 'train', 'eval', 'test' 모드 표시\n",
        "        \"\"\"\n",
        "        new_data = {\n",
        "            \"timestamp\": [now_str()],\n",
        "            \"Model\": [model_name],\n",
        "            \"Mode\": [mode],\n",
        "            \"Max_Epochs\": [max_epochs],\n",
        "            \"Epoch\": [epoch_index],\n",
        "            \"Train_Loss\": [round(train_loss, 6) if train_loss is not None else None],\n",
        "            \"Val_Loss\": [round(val_loss, 6) if val_loss is not None else None],\n",
        "            \"Test_Loss\": [round(test_loss, 6) if test_loss is not None else None],\n",
        "            \"mAP50\": [round(mAP50, 4) if mAP50 is not None else None],\n",
        "            \"mAP50_95\": [round(mAP50_95, 4) if mAP50_95 is not None else None],\n",
        "            \"Precision\": [round(precision, 4) if precision is not None else None],\n",
        "            \"Recall\": [round(recall, 4) if recall is not None else None],\n",
        "            \"Total_Detections\": [total_detections if total_detections is not None else None],\n",
        "            \"Avg_Confidence\": [round(avg_confidence, 4) if avg_confidence is not None else None],\n",
        "            \"Learning_Rate\": [round(current_lr, 8) if current_lr is not None else None],\n",
        "        }\n",
        "        filename = RESULT_CSV\n",
        "        lock_filename = filename + \".lock\"\n",
        "        new_df = pd.DataFrame(new_data)\n",
        "        try:\n",
        "            makedirs(os.path.dirname(filename))\n",
        "            lock = FileLock(lock_filename, timeout=10)\n",
        "            with lock:\n",
        "                if os.path.exists(filename):\n",
        "                    try:\n",
        "                        existing_df = pd.read_csv(filename)\n",
        "                        updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
        "                        updated_df.to_csv(filename, index=False)\n",
        "                    except:\n",
        "                        new_df.to_csv(filename, index=False)\n",
        "                else:\n",
        "                    new_df.to_csv(filename, index=False)\n",
        "        except Exception as e:\n",
        "            print(f\"CSV 저장 중 오류 발생: {e}\")\n",
        "            OpLog(f\"Error saving CSV: {e}\")\n",
        "    def _visualize_results(self, epoch, max_epochs, predictions, mode=\"eval\", test_img_dir=None):\n",
        "        \"\"\"\n",
        "        검증/테스트 결과 시각화 (공통 구현)\n",
        "        Args:\n",
        "            epoch: 현재 에포크 번호\n",
        "            max_epochs: 전체 에포크 수\n",
        "            predictions: 예측 결과 (dict 또는 list)\n",
        "            mode: 'eval' 또는 'test'\n",
        "            test_img_dir: 테스트 이미지 디렉토리 (mode='test'일 때 필요)\n",
        "        \"\"\"\n",
        "        # Corrected results_dir path: removes redundant 'oraldrug' segment\n",
        "        results_dir = os.path.join(BASE_DIR, \"results\", self.getMyName())\n",
        "        makedirs(results_dir)\n",
        "        if mode == \"eval\":\n",
        "            # 검증 모드: 메트릭 표시\n",
        "            fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "            # predictions가 dict 형태일 경우 (메트릭 정보)\n",
        "            if isinstance(predictions, dict):\n",
        "                metrics_text = f\"Epoch {epoch}/{max_epochs} - Validation Metrics\\n\\n\"\n",
        "                mAP50_val = predictions.get('mAP50')\n",
        "                mAP50_95_val = predictions.get('mAP50_95')\n",
        "                precision_val = predictions.get('precision')\n",
        "                recall_val = predictions.get('recall')\n",
        "\n",
        "                metrics_text += f\"mAP50: {mAP50_val:.4f}\\n\" if mAP50_val is not None else \"mAP50: N/A\\n\"\n",
        "                metrics_text += f\"mAP50-95: {mAP50_95_val:.4f}\\n\" if mAP50_95_val is not None else \"mAP50-95: N/A\\n\"\n",
        "                metrics_text += f\"Precision: {precision_val:.4f}\\n\" if precision_val is not None else \"Precision: N/A\\n\"\n",
        "                metrics_text += f\"Recall: {recall_val:.4f}\" if recall_val is not None else \"Recall: N/A\"\n",
        "            else:\n",
        "                # predictions가 list 형태일 경우\n",
        "                num_preds = len(predictions) if isinstance(predictions, list) else 0\n",
        "                metrics_text = f\"Epoch {epoch}/{max_epochs} - Validation\\n\\n\"\n",
        "                metrics_text += f\"Total Predictions: {num_preds}\"\n",
        "            ax.text(\n",
        "                0.5,\n",
        "                0.5,\n",
        "                metrics_text,\n",
        "                ha=\"center\",\n",
        "                va=\"center\",\n",
        "                fontsize=14,\n",
        "                family=\"monospace\",\n",
        "            )\n",
        "            ax.axis(\"off\")\n",
        "            save_path = os.path.join(results_dir, f\"eval_epoch{epoch:03d}.png\")\n",
        "            plt.savefig(save_path, bbox_inches=\"tight\", dpi=100)\n",
        "            plt.close()\n",
        "            OpLog(f\"검증 결과 저장: {save_path}\", bLines=False)\n",
        "        elif mode == \"test\":\n",
        "            # 테스트 모드: 예측 결과 샘플 시각화 (최대 6개)\n",
        "            if not isinstance(predictions, list):\n",
        "                OpLog(\"Warning: predictions가 list 형태가 아닙니다.\", bLines=False)\n",
        "                return\n",
        "\n",
        "            num_samples = min(6, len(predictions))\n",
        "            if num_samples == 0:\n",
        "                return\n",
        "            fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "            axes = axes.flatten()\n",
        "            for i in range(num_samples):\n",
        "                pred = predictions[i]\n",
        "\n",
        "                # pred가 dict이고 filename 키를 가지고 있는지 확인\n",
        "                if isinstance(pred, dict) and \"filename\" in pred:\n",
        "                    filename = pred[\"filename\"]\n",
        "                    num_boxes = len(pred.get(\"boxes\", []))\n",
        "\n",
        "                    if test_img_dir and os.path.exists(test_img_dir):\n",
        "                        img_path = os.path.join(test_img_dir, filename)\n",
        "                        if os.path.exists(img_path):\n",
        "                            img = Image.open(img_path).convert(\"RGB\")\n",
        "                            axes[i].imshow(img)\n",
        "                            axes[i].set_title(\n",
        "                                f\"{filename}\\nDetections: {num_boxes}\"\n",
        "                            )\n",
        "                            axes[i].axis(\"off\")\n",
        "                        else:\n",
        "                            axes[i].text(\n",
        "                                0.5,\n",
        "                                0.5,\n",
        "                                f\"Image not found:\\n{filename}\",\n",
        "                                ha=\"center\",\n",
        "                                va=\"center\",\n",
        "                            )\n",
        "                            axes[i].axis(\"off\")\n",
        "                    else:\n",
        "                        # test_img_dir이 없으면 텍스트만 표시\n",
        "                        axes[i].text(\n",
        "                            0.5,\n",
        "                            0.5,\n",
        "                            f\"{filename}\\nDetections: {num_boxes}\",\n",
        "                            ha=\"center\",\n",
        "                            va=\"center\",\n",
        "                        )\n",
        "                        axes[i].set_title(f\"Sample {i+1}\")\n",
        "                        axes[i].axis(\"off\")\n",
        "                else:\n",
        "                    # 예측 결과가 dict 형태가 아닐 경우 기본 표시\n",
        "                    num_detections = len(pred.get(\"boxes\", [])) if isinstance(pred, dict) else 0\n",
        "                    avg_score = pred.get(\"scores\", torch.tensor([])).mean().item() if isinstance(pred, dict) and len(pred.get(\"scores\", [])) > 0 else 0\n",
        "                    axes[i].text(\n",
        "                        0.5,\n",
        "                        0.5,\n",
        "                        f\"Detections: {num_detections}\\nAvg Score: {avg_score:.3f}\",\n",
        "                        ha=\"center\",\n",
        "                        va=\"center\",\n",
        "                        fontsize=12,\n",
        "                    )\n",
        "                    axes[i].set_title(f\"Sample {i+1}\")\n",
        "                    axes[i].axis(\"off\")\n",
        "\n",
        "            # 빈 서브플롯 숨기기\n",
        "            for i in range(num_samples, 6):\n",
        "                axes[i].axis(\"off\")\n",
        "            plt.suptitle(\n",
        "                f\"Epoch {epoch}/{max_epochs} - Test Predictions (Sample)\", fontsize=16\n",
        "            )\n",
        "            plt.tight_layout()\n",
        "            save_path = os.path.join(results_dir, f\"test_epoch{epoch:03d}.png\")\n",
        "            plt.savefig(save_path, bbox_inches=\"tight\", dpi=100)\n",
        "            plt.close()\n",
        "            OpLog(f\"테스트 결과 저장: {save_path}\", bLines=False)\n",
        "    def _save_eval_metrics(self, epoch, max_epochs, metrics_dict, train_loss=None, val_loss=None, current_lr=None):\n",
        "        \"\"\"\n",
        "        검증 메트릭 저장 및 시각화 (공통 헬퍼)\n",
        "\n",
        "        Args:\n",
        "            epoch: 현재 에포크\n",
        "            max_epochs: 전체 에포크 수\n",
        "            metrics_dict: 메트릭 딕셔너리 {'mAP50': float, 'mAP50_95': float, 'precision': float, 'recall': float}\n",
        "            train_loss: 학습 손실\n",
        "            val_loss: 검증 손실\n",
        "            current_lr: 현재 학습률\n",
        "        \"\"\"\n",
        "        # CSV에 평가 메트릭 저장\n",
        "        self.save_metrics_to_csv(\n",
        "            model_name=self.getMyName(),\n",
        "            epoch_index=epoch,\n",
        "            max_epochs=max_epochs,\n",
        "            train_loss=train_loss,\n",
        "            val_loss=val_loss,\n",
        "            current_lr=current_lr,\n",
        "            mode=\"eval\",\n",
        "            mAP50=metrics_dict.get('mAP50'),\n",
        "            mAP50_95=metrics_dict.get('mAP50_95'),\n",
        "            precision=metrics_dict.get('precision'),\n",
        "            recall=metrics_dict.get('recall'),\n",
        "            total_detections=metrics_dict.get('total_detections'),\n",
        "            avg_confidence=metrics_dict.get('avg_confidence'),\n",
        "        )\n",
        "\n",
        "        # 시각화\n",
        "        self._visualize_results(epoch, max_epochs, metrics_dict, mode=\"eval\")\n",
        "    def _save_test_metrics(self, epoch, max_epochs, predictions, test_img_dir, train_loss=None, current_lr=None):\n",
        "        \"\"\"\n",
        "        테스트 메트릭 저장 및 시각화 (공통 헬퍼)\n",
        "\n",
        "        Args:\n",
        "            epoch: 현재 에포크\n",
        "            max_epochs: 전체 에포크 수\n",
        "            predictions: 예측 결과 리스트 [{'boxes': array, 'scores': array, 'labels': array, 'filename': str}, ...]\n",
        "            test_img_dir: 테스트 이미지 디렉토리\n",
        "            train_loss: 학습 손실\n",
        "            current_lr: 현재 학습률\n",
        "        \"\"\"\n",
        "        # 통계 계산\n",
        "        total_detections = sum(len(p[\"boxes\"]) for p in predictions)\n",
        "\n",
        "        # 평균 신뢰도 계산 (numpy array와 torch tensor 모두 지원)\n",
        "        confidences = []\n",
        "        for p in predictions:\n",
        "            scores = p.get(\"scores\", [])\n",
        "            if len(scores) > 0:\n",
        "                if hasattr(scores, 'mean'):  # numpy array or torch tensor\n",
        "                    conf = float(scores.mean())\n",
        "                else:\n",
        "                    conf = sum(scores) / len(scores)\n",
        "                confidences.append(conf)\n",
        "\n",
        "        avg_confidence = sum(confidences) / max(len(confidences), 1) if confidences else 0.0\n",
        "        test_loss = 1.0 - avg_confidence\n",
        "\n",
        "        # 정밀도/재현율 근사 계산\n",
        "        precision = avg_confidence if avg_confidence > 0 else None\n",
        "        recall = avg_confidence * 0.9 if avg_confidence > 0 else None\n",
        "\n",
        "        OpLog(\n",
        "            f\"Epoch [{epoch}/{max_epochs}] - Test: {len(predictions)} images, {total_detections} detections, Avg Conf: {avg_confidence:.4f}\",\n",
        "            bLines=True,\n",
        "        )\n",
        "\n",
        "        # CSV에 테스트 메트릭 저장\n",
        "        self.save_metrics_to_csv(\n",
        "            model_name=self.getMyName(),\n",
        "            epoch_index=epoch,\n",
        "            max_epochs=max_epochs,\n",
        "            train_loss=train_loss,\n",
        "            test_loss=test_loss,\n",
        "            current_lr=current_lr,\n",
        "            mode=\"test\",\n",
        "            total_detections=total_detections,\n",
        "            avg_confidence=avg_confidence,\n",
        "            precision=precision,\n",
        "            recall=recall,\n",
        "        )\n",
        "\n",
        "        # 시각화 (최대 6개 샘플)\n",
        "        self._visualize_results(epoch, max_epochs, predictions[:6], mode=\"test\", test_img_dir=test_img_dir)\n",
        "    def fit(\n",
        "        self,\n",
        "        gubun=\"freeze\",\n",
        "        train_loader=None,\n",
        "        val_loader=None,\n",
        "        test_loader=None,\n",
        "        epochs=50,\n",
        "        imgsz=640,\n",
        "        batch_size=16,\n",
        "        lr=0.001,\n",
        "        patience=10,\n",
        "    ):\n",
        "        \"\"\"모델 학습 - 서브클래스에서 구현 필요\n",
        "        Args:\n",
        "            gubun: 최적화 방식 ('freeze', 'partial', 'all') - FasterRCNN에서 사용\n",
        "            train_loader: 학습 데이터로더\n",
        "            val_loader: 검증 데이터로더\n",
        "            test_loader: 테스트 데이터로더\n",
        "            epochs: 학습 에포크 수\n",
        "            imgsz: 입력 이미지 크기 - YOLOv8에서 사용\n",
        "            batch_size: 배치 크기 - YOLOv8에서 사용\n",
        "            lr: 학습률\n",
        "            patience: Early stopping patience (검증 손실이 개선되지 않을 때 대기할 에포크 수)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"fit must be implemented by subclass\")\n",
        "    def evalModel(self, val_loader, epoch, max_epochs):\n",
        "        \"\"\"검증 모드 - 서브클래스에서 구현 필요\"\"\"\n",
        "        raise NotImplementedError(\"evalModel must be implemented by subclass\")\n",
        "    def testModel(self, test_loader, epoch, max_epochs):\n",
        "        \"\"\"테스트 모드 - 서브클래스에서 구현 필요\"\"\"\n",
        "        raise NotImplementedError(\"testMode must be implemented by subclass\")\n",
        "\n",
        "    def CreateSubmission(self, predictions, test_img_dir, class_names, save_image_num=10):\n",
        "        \"\"\"제출 파일 생성 (공통 구현)\n",
        "\n",
        "        Args:\n",
        "            predictions: 예측 결과 리스트 [{'boxes': array, 'scores': array, 'labels': array, 'filename': str}, ...]\n",
        "            test_img_dir: 테스트 이미지 디렉토리\n",
        "            class_names: 클래스 이름 리스트 (category_id 순서대로)\n",
        "            save_image_num: 저장할 이미지 개수\n",
        "        \"\"\"\n",
        "        import matplotlib.patches as patches\n",
        "        import re\n",
        "\n",
        "        # 타임스탬프로 디렉토리 생성\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "        submission_dir = os.path.join(SUBMISSTION_DIR, f\"submission{timestamp}\")\n",
        "        makedirs(submission_dir)\n",
        "\n",
        "        csv_path = os.path.join(submission_dir, f\"submission{timestamp}.csv\")\n",
        "\n",
        "        OpLog(f\"Submission 생성 시작: {submission_dir}\", bLines=True)\n",
        "\n",
        "        # CSV 데이터 준비\n",
        "        csv_data = []\n",
        "        annotation_id = 1\n",
        "\n",
        "        for pred in predictions:\n",
        "            # 이미지 ID 추출 (파일명에서 숫자 추출)\n",
        "            filename = pred.get('filename', '')\n",
        "            match = re.search(r'(\\d+)', filename)\n",
        "            image_id = int(match.group(1)) if match else annotation_id\n",
        "\n",
        "            boxes = pred.get('boxes', [])\n",
        "            scores = pred.get('scores', [])\n",
        "            labels = pred.get('labels', [])\n",
        "\n",
        "            # numpy array를 리스트로 변환\n",
        "            if hasattr(boxes, 'tolist'):\n",
        "                boxes = boxes.tolist() if len(boxes) > 0 else []\n",
        "            if hasattr(scores, 'tolist'):\n",
        "                scores = scores.tolist() if len(scores) > 0 else []\n",
        "            if hasattr(labels, 'tolist'):\n",
        "                labels = labels.tolist() if len(labels) > 0 else []\n",
        "\n",
        "            # 각 탐지 결과를 CSV에 추가\n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "                # box 형식: [x1, y1, x2, y2] -> [x, y, w, h]\n",
        "                if len(box) == 4:\n",
        "                    x1, y1, x2, y2 = box\n",
        "                    bbox_x = int(x1)\n",
        "                    bbox_y = int(y1)\n",
        "                    bbox_w = int(x2 - x1)\n",
        "                    bbox_h = int(y2 - y1)\n",
        "\n",
        "                    # label은 인덱스이므로 class_names에서 실제 category_id 가져오기\n",
        "                    label_idx = int(label)\n",
        "                    actual_category_id = int(class_names[label_idx]) if label_idx < len(class_names) else label_idx\n",
        "\n",
        "                    csv_data.append({\n",
        "                        'annotation_id': annotation_id,\n",
        "                        'image_id': image_id,\n",
        "                        'category_id': actual_category_id,\n",
        "                        'bbox_x': bbox_x,\n",
        "                        'bbox_y': bbox_y,\n",
        "                        'bbox_w': bbox_w,\n",
        "                        'bbox_h': bbox_h,\n",
        "                        'score': round(float(score), 2)\n",
        "                    })\n",
        "                    annotation_id += 1\n",
        "\n",
        "        # CSV 파일 저장\n",
        "        df = pd.DataFrame(csv_data)\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        OpLog(f\"CSV 파일 저장: {csv_path} ({len(csv_data)}개 탐지)\", bLines=False)\n",
        "\n",
        "        # 이미지 시각화 및 저장\n",
        "        num_to_save = min(save_image_num, len(predictions))\n",
        "        OpLog(f\"이미지 시각화 시작: {num_to_save}개\", bLines=False)\n",
        "\n",
        "        for idx in range(num_to_save):\n",
        "            pred = predictions[idx]\n",
        "            filename = pred.get('filename', f'image_{idx}.jpg')\n",
        "            img_path = os.path.join(test_img_dir, filename)\n",
        "\n",
        "            if not os.path.exists(img_path):\n",
        "                OpLog(f\"이미지를 찾을 수 없습니다: {img_path}\", bLines=False)\n",
        "                continue\n",
        "\n",
        "            # 이미지 로드\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            fig, ax = plt.subplots(1, figsize=(12, 12))\n",
        "            ax.imshow(img)\n",
        "\n",
        "            boxes = pred.get('boxes', [])\n",
        "            scores = pred.get('scores', [])\n",
        "            labels = pred.get('labels', [])\n",
        "\n",
        "            # numpy array 변환\n",
        "            if hasattr(boxes, 'tolist'):\n",
        "                boxes = boxes.tolist() if len(boxes) > 0 else []\n",
        "            if hasattr(scores, 'tolist'):\n",
        "                scores = scores.tolist() if len(scores) > 0 else []\n",
        "            if hasattr(labels, 'tolist'):\n",
        "                labels = labels.tolist() if len(labels) > 0 else []\n",
        "\n",
        "            # 각 박스 그리기\n",
        "            for box, score, label in zip(boxes, scores, labels):\n",
        "                if len(box) == 4:\n",
        "                    x1, y1, x2, y2 = box\n",
        "                    width = x2 - x1\n",
        "                    height = y2 - y1\n",
        "\n",
        "                    # 박스 그리기\n",
        "                    rect = patches.Rectangle(\n",
        "                        (x1, y1), width, height,\n",
        "                        linewidth=2, edgecolor='red', facecolor='none'\n",
        "                    )\n",
        "                    ax.add_patch(rect)\n",
        "\n",
        "                    # 카테고리 ID와 약품 이름 표시\n",
        "                    label_idx = int(label)\n",
        "                    category_name = str(class_names[label_idx]) if label_idx < len(class_names) else str(label_idx)\n",
        "\n",
        "                    # 레이블 텍스트\n",
        "                    label_text = f\"Cat:{category_name}\\nScore:{score:.2f}\"\n",
        "\n",
        "                    # 텍스트 배경 박스\n",
        "                    ax.text(\n",
        "                        x1, y1 - 5,\n",
        "                        label_text,\n",
        "                        bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
        "                        fontsize=8,\n",
        "                        color='black',\n",
        "                        verticalalignment='top'\n",
        "                    )\n",
        "\n",
        "            ax.axis('off')\n",
        "            ax.set_title(f\"{filename} - {len(boxes)} detections\", fontsize=14, pad=10)\n",
        "\n",
        "            # 이미지 저장\n",
        "            save_img_path = os.path.join(submission_dir, f\"result_{idx+1:03d}_{filename}\")\n",
        "            plt.savefig(save_img_path, bbox_inches='tight', dpi=150)\n",
        "            plt.close()\n",
        "\n",
        "            OpLog(f\"  [{idx+1}/{num_to_save}] {filename} 저장 완료\", bLines=False)\n",
        "\n",
        "        OpLog(f\"Submission 생성 완료: {submission_dir}\", bLines=True)\n",
        "        OpLog(f\"  - CSV: {csv_path}\", bLines=False)\n",
        "        OpLog(f\"  - 이미지: {num_to_save}개\", bLines=False)\n",
        "\n",
        "        return csv_path, submission_dir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1Ll9o_bA4xk"
      },
      "source": [
        "════════════════════════════════════════<br>\n",
        "▣ 05. YOLOv8 모델 정의<br>\n",
        "════════════════════════════════════════"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pwvlf4m5A4xk"
      },
      "outputs": [],
      "source": [
        "class YOLOv8Model(BaseModel):\n",
        "    \"\"\"\n",
        "    YOLOv8 기반 객체 탐지 모델\n",
        "    - Ultralytics YOLOv8 사용\n",
        "    - 객체 탐지 및 분류 동시 수행\n",
        "    \"\"\"\n",
        "    def __init__(self, model_size=\"n\", num_classes=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model_size: YOLOv8 모델 크기 ('n', 's', 'm', 'l', 'x')\n",
        "            num_classes: 클래스 수 (필수 파라미터)\n",
        "        \"\"\"\n",
        "        super(YOLOv8Model, self).__init__()\n",
        "        self.model_size = model_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # YOLOv8 모델 초기화 (사전 학습된 가중치 사용)\n",
        "        model_path = f\"yolov8{model_size}.pt\"\n",
        "\n",
        "        # 모델 파일이 손상된 경우 삭제하고 재다운로드\n",
        "        if os.path.exists(model_path):\n",
        "            try:\n",
        "                # 파일 검증 시도\n",
        "                test_model = YOLO(model_path)\n",
        "                self.model = test_model\n",
        "                OpLog(f\"YOLOv8{model_size} 모델 로드 완료\", bLines=False)\n",
        "            except Exception as e:\n",
        "                OpLog(f\"YOLOv8 모델 파일 손상 감지, 재다운로드 중...: {e}\", bLines=True)\n",
        "                try:\n",
        "                    os.remove(model_path)\n",
        "                    OpLog(f\"손상된 모델 파일 삭제: {model_path}\", bLines=False)\n",
        "                except:\n",
        "                    pass\n",
        "                self.model = YOLO(model_path)  # 자동 재다운로드\n",
        "        else:\n",
        "            OpLog(f\"YOLOv8{model_size} 모델 다운로드 중...\", bLines=True)\n",
        "            self.model = YOLO(model_path)  # 자동 다운로드\n",
        "\n",
        "        self.optimizer = None\n",
        "        self.lr_scheduler = None\n",
        "    def getMyName(self):\n",
        "        return f\"YOLOv8Model_{self.model_size}\"\n",
        "    def getOptimizer(self, lr=0.001, gubun=\"freeze\"):\n",
        "        \"\"\"\n",
        "        YOLOv8 모델의 optimizer를 반환\n",
        "        YOLOv8는 내부적으로 optimizer를 관리하므로 이 메서드는 인터페이스 통일을 위해 제공됨\n",
        "        Args:\n",
        "            lr: 학습률\n",
        "            gubun: 최적화 방식 ('freeze', 'partial', 'all')\n",
        "        Returns:\n",
        "            optimizer: YOLOv8는 내부 optimizer를 사용하므로 None 반환\n",
        "        \"\"\"\n",
        "        OpLog(\n",
        "            f\"YOLOv8는 내부 optimizer를 사용합니다. (lr={lr}, mode={gubun})\",\n",
        "            bLines=False,\n",
        "        )\n",
        "        # YOLOv8는 ultralytics 내부에서 optimizer를 자동 관리\n",
        "        return None\n",
        "    def preJob(self, annotation_dir, image_dir, yaml_file, yaml_label_dir):\n",
        "        \"\"\"\n",
        "        전처리 작업: YOLO YAML 파일, 클래스 매핑, YOLO 형식 레이블 생성\n",
        "\n",
        "        Args:\n",
        "            annotation_dir: 어노테이션 디렉토리\n",
        "            image_dir: 이미지 디렉토리\n",
        "            yaml_file: YAML 파일 경로\n",
        "            yaml_label_dir: YOLO 레이블 디렉토리\n",
        "        \"\"\"\n",
        "        import yaml\n",
        "        class_mapping_file = os.path.join(os.path.dirname(yaml_file), \"class_mapping.json\")\n",
        "\n",
        "        # 기존 YAML 파일과 labels 삭제 (새로 생성하기 위해)\n",
        "        if os.path.exists(yaml_file):\n",
        "            OpLog(f\"기존 YAML 파일이 있으므로 preJob종료: {yaml_file}\", bLines=True)\n",
        "            return\n",
        "\n",
        "        if os.path.exists(yaml_label_dir):\n",
        "            try:\n",
        "                import shutil\n",
        "                shutil.rmtree(yaml_label_dir)\n",
        "                OpLog(f\"기존 labels 디렉토리 삭제: {yaml_label_dir}\", bLines=False)\n",
        "            except Exception as e:\n",
        "                OpLog(f\"labels 디렉토리 삭제 실패: {e}\", bLines=False)\n",
        "        OpLog(\"YOLO 데이터셋 준비 시작\", bLines=True)\n",
        "\n",
        "        # get_class_mapping 함수 사용하여 클래스 정보 가져오기\n",
        "        class_dirs, class_to_idx, idx_to_class, class_names = get_class_mapping(\n",
        "            annotation_dir\n",
        "        )\n",
        "\n",
        "        # 클래스 매핑 정보 저장 (category_id: index)\n",
        "        class_mapping = {}\n",
        "        for category_id in class_names:\n",
        "            class_mapping[str(category_id)] = {\"index\": class_to_idx[category_id]}\n",
        "\n",
        "        # 클래스 매핑 JSON 파일 저장\n",
        "        with open(class_mapping_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(class_mapping, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # JSON annotation을 YOLO 형식(.txt)으로 변환\n",
        "        OpLog(\"JSON annotation을 YOLO 형식으로 변환 중...\", bLines=False)\n",
        "\n",
        "        # 레이블 파일은 이미지와 같은 디렉토리에 생성 (YOLO 요구사항)\n",
        "        # yolo_label_dir는 사용하지 않고 image_dir에 직접 생성\n",
        "        # makedirs(yaml_label_dir)  # 사용 안 함\n",
        "\n",
        "        # 모든 이미지별 annotation을 수집하기 위한 딕셔너리\n",
        "        # key: 이미지 파일명, value: {'width': int, 'height': int, 'annotations': [{'bbox': [], 'category_id': int}]}\n",
        "        image_annotations_dict = {}\n",
        "\n",
        "        # 1단계: 모든 JSON 파일에서 annotation 수집\n",
        "        total_annotations = 0\n",
        "        for category_id, json_paths in class_dirs:\n",
        "            class_id = class_to_idx[category_id]\n",
        "            for json_path in json_paths:\n",
        "                try:\n",
        "                    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                        data = json.load(f)\n",
        "                    # 각 이미지에 대한 annotation 처리\n",
        "                    if \"images\" in data and \"annotations\" in data:\n",
        "                        for img_info in data[\"images\"]:\n",
        "                            img_filename = img_info[\"file_name\"]\n",
        "                            img_id = img_info[\"id\"]\n",
        "                            img_width = img_info.get(\"width\", 640)\n",
        "                            img_height = img_info.get(\"height\", 640)\n",
        "                            # 해당 이미지의 annotation 찾기\n",
        "                            img_annotations = [\n",
        "                                ann\n",
        "                                for ann in data[\"annotations\"]\n",
        "                                if ann.get(\"image_id\") == img_id\n",
        "                            ]\n",
        "                            if img_annotations:\n",
        "                                # 이미지별로 annotation 수집\n",
        "                                if img_filename not in image_annotations_dict:\n",
        "                                    image_annotations_dict[img_filename] = {\n",
        "                                        'width': img_width,\n",
        "                                        'height': img_height,\n",
        "                                        'annotations': []\n",
        "                                    }\n",
        "\n",
        "                                # annotation 추가 (중복 방지)\n",
        "                                for ann in img_annotations:\n",
        "                                    bbox = ann.get(\"bbox\", [])\n",
        "                                    ann_category_id = ann.get(\"category_id\", category_id)\n",
        "\n",
        "                                    if len(bbox) == 4:\n",
        "                                        # 중복 체크: bbox 좌표와 category_id가 모두 같은 경우만 중복으로 판단\n",
        "                                        is_duplicate = False\n",
        "                                        for existing_ann in image_annotations_dict[img_filename]['annotations']:\n",
        "                                            if (abs(existing_ann['bbox'][0] - bbox[0]) < 0.01 and\n",
        "                                                abs(existing_ann['bbox'][1] - bbox[1]) < 0.01 and\n",
        "                                                abs(existing_ann['bbox'][2] - bbox[2]) < 0.01 and\n",
        "                                                abs(existing_ann['bbox'][3] - bbox[3]) < 0.01 and\n",
        "                                                existing_ann['category_id'] == ann_category_id):\n",
        "                                                is_duplicate = True\n",
        "                                                break\n",
        "\n",
        "                                        if not is_duplicate:\n",
        "                                            image_annotations_dict[img_filename]['annotations'].append({\n",
        "                                                'bbox': bbox,\n",
        "                                                'category_id': ann_category_id\n",
        "                                            })\n",
        "                                            total_annotations += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    OpLog(f\"Error reading {json_path}: {e}\", bLines=False)\n",
        "        OpLog(f\"총 {total_annotations}개 annotation 수집 완료 ({len(image_annotations_dict)}개 이미지)\", bLines=False)\n",
        "\n",
        "        # 2단계: 수집된 annotation을 YOLO 형식 파일로 저장\n",
        "        converted_count = 0\n",
        "        for img_filename, img_data in image_annotations_dict.items():\n",
        "            try:\n",
        "                img_width = img_data['width']\n",
        "                img_height = img_data['height']\n",
        "                annotations = img_data['annotations']\n",
        "\n",
        "                if len(annotations) == 0:\n",
        "                    OpLog(f\"Warning: {img_filename}에 annotation이 없습니다.\", bLines=False)\n",
        "                    continue\n",
        "\n",
        "                # YOLO 형식 레이블 파일 생성 (이미지와 같은 디렉토리에 생성)\n",
        "                label_filename = os.path.splitext(img_filename)[0] + \".txt\"\n",
        "                label_path = os.path.join(image_dir, label_filename)  # yaml_label_dir 대신 image_dir 사용\n",
        "                with open(label_path, \"w\", encoding=\"utf-8\") as lf:\n",
        "                    for ann in annotations:\n",
        "                        bbox = ann['bbox']\n",
        "                        x, y, w, h = bbox\n",
        "                        # YOLO 형식으로 변환: [x_center, y_center, width, height] (0~1 정규화)\n",
        "                        x_center = (x + w / 2) / img_width\n",
        "                        y_center = (y + h / 2) / img_height\n",
        "                        norm_width = w / img_width\n",
        "                        norm_height = h / img_height\n",
        "                        # category_id를 YOLO class index(0-based)로 변환\n",
        "                        ann_category_id = ann['category_id']\n",
        "                        try:\n",
        "                            ann_category_id = int(ann_category_id)\n",
        "                        except (ValueError, TypeError):\n",
        "                            ann_category_id = class_names[0]\n",
        "\n",
        "                        try:\n",
        "                            yolo_class_idx = class_names.index(ann_category_id)\n",
        "                        except ValueError:\n",
        "                            OpLog(f\"Warning: category_id {ann_category_id} not in class_names, skipping\", bLines=False)\n",
        "                            continue\n",
        "\n",
        "                        lf.write(\n",
        "                            f\"{yolo_class_idx} {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}\\n\"\n",
        "                        )\n",
        "                converted_count += 1\n",
        "\n",
        "                # 디버깅: 여러 객체가 있는 이미지 로그\n",
        "                if len(annotations) > 1:\n",
        "                    OpLog(f\"{img_filename}: {len(annotations)}개 객체 저장됨\", bLines=False)\n",
        "\n",
        "            except Exception as e:\n",
        "                OpLog(f\"Error writing label file for {img_filename}: {e}\", bLines=False)\n",
        "        OpLog(f\"YOLO 레이블 변환 완료: {converted_count}개 파일\", bLines=False)\n",
        "\n",
        "        # YAML 데이터 구조 생성 (모든 경로를 절대 경로로 사용)\n",
        "        # names를 리스트로 설정 (YOLO는 0-based index 사용)\n",
        "        # class 0 = category_id class_names[0], class 1 = category_id class_names[1], ...\n",
        "\n",
        "        # test_img_dir 계산 (image_dir의 부모 디렉토리에서 test_images 찾기)\n",
        "        data_root = os.path.dirname(image_dir)\n",
        "        test_img_dir = os.path.join(data_root, \"test_images\")\n",
        "\n",
        "        yaml_data = {\n",
        "            \"path\": os.path.abspath(data_root).replace(\"\\\\\", \"/\"),\n",
        "            \"train\": os.path.abspath(image_dir).replace(\"\\\\\", \"/\"),\n",
        "            \"val\": os.path.abspath(image_dir).replace(\"\\\\\", \"/\"),\n",
        "            \"test\": os.path.abspath(test_img_dir).replace(\"\\\\\", \"/\"),\n",
        "            \"nc\": len(class_names),\n",
        "            \"names\": [str(cat_id) for cat_id in class_names],  # 리스트 형태: ['1899', '2482', '3350', ...]\n",
        "        }\n",
        "\n",
        "        # YAML 파일 저장\n",
        "        makedirs(os.path.dirname(yaml_file))\n",
        "        with open(yaml_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            yaml.dump(\n",
        "                yaml_data,\n",
        "                f,\n",
        "                default_flow_style=False,\n",
        "                allow_unicode=True,\n",
        "                sort_keys=False,\n",
        "            )\n",
        "        OpLog(f\"YAML 파일 생성 완료: {yaml_file}\", bLines=False)\n",
        "        OpLog(f\"  - train: {yaml_data['train']}\", bLines=False)\n",
        "        OpLog(f\"  - val: {yaml_data['val']}\", bLines=False)\n",
        "        OpLog(f\"  - test: {yaml_data['test']}\", bLines=False)\n",
        "        OpLog(f\"클래스 매핑 파일 생성 완료: {class_mapping_file}\", bLines=False)\n",
        "        OpLog(f\"총 클래스 수: {len(class_names)}\", bLines=False)\n",
        "    def fit(\n",
        "        self,\n",
        "        annotation_dir,\n",
        "        image_dir,\n",
        "        yaml_file,\n",
        "        yaml_label_dir,\n",
        "        test_img_dir,\n",
        "        gubun=\"freeze\",\n",
        "        train_loader=None,\n",
        "        val_loader=None,\n",
        "        test_loader=None,\n",
        "        epochs=50,\n",
        "        imgsz=640,\n",
        "        batch_size=16,\n",
        "        lr=0.001,\n",
        "        patience=10,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        YOLOv8 모델 학습 (BaseModel 인터페이스 준수)\n",
        "        Args:\n",
        "            annotation_dir: 어노테이션 디렉토리\n",
        "            image_dir: 학습 이미지 디렉토리\n",
        "            yaml_file: YAML 파일 경로\n",
        "            yaml_label_dir: YOLO 레이블 디렉토리\n",
        "            test_img_dir: 테스트 이미지 디렉토리\n",
        "            gubun: 최적화 방식 (YOLOv8는 사용하지 않음, 인터페이스 통일용)\n",
        "            train_loader: 학습 데이터로더 (YOLOv8는 내부적으로 사용하지 않지만 인터페이스 통일)\n",
        "            val_loader: 검증 데이터로더\n",
        "            test_loader: 테스트 데이터로더\n",
        "            epochs: 학습 에포크 수\n",
        "            imgsz: 입력 이미지 크기\n",
        "            batch_size: 배치 크기\n",
        "            lr: 학습률 (YOLOv8는 내부적으로 관리)\n",
        "            patience: Early stopping patience (과적합 방지)\n",
        "        \"\"\"\n",
        "        OpLog(f\"YOLOv8{self.model_size} 모델 학습 시작\", bLines=True)\n",
        "\n",
        "        # YAML 파일과 labels를 항상 새로 생성\n",
        "        OpLog(\"YAML 파일 및 레이블 재생성 중...\", bLines=True)\n",
        "        self.preJob(annotation_dir, image_dir, yaml_file, yaml_label_dir)\n",
        "\n",
        "        # YOLOv8 학습 시작 (YOLOv8는 내부적으로 전체 에포크를 학습하고 검증까지 자동 수행)\n",
        "        data_root = os.path.dirname(image_dir)\n",
        "        results = self.model.train(\n",
        "            data=yaml_file,\n",
        "            epochs=epochs,\n",
        "            imgsz=imgsz,\n",
        "            batch=batch_size,\n",
        "            device=DEVICE_TYPE,\n",
        "            project=os.path.join(data_root, \"yolo_results\"),\n",
        "            name=f\"yolov8{self.model_size}_train\",\n",
        "            exist_ok=True,\n",
        "            patience=patience,  # Early stopping (과적합시 훈련 종료)\n",
        "            save=True,\n",
        "            plots=True,\n",
        "        )\n",
        "        OpLog(f\"YOLOv8 학습 완료!\", bLines=True)\n",
        "\n",
        "        # YOLOv8 best/last 모델 파일 복사\n",
        "        try:\n",
        "            results_dir = os.path.join(data_root, \"yolo_results\", f\"yolov8{self.model_size}_train\")\n",
        "            weights_dir = os.path.join(results_dir, \"weights\")\n",
        "            best_pt = os.path.join(weights_dir, \"best.pt\")\n",
        "            last_pt = os.path.join(weights_dir, \"last.pt\")\n",
        "\n",
        "            makedirs(MODEL_FILES)\n",
        "            if os.path.exists(best_pt):\n",
        "                shutil.copy(best_pt, os.path.join(MODEL_FILES, \"yolobest.pt\"))\n",
        "                OpLog(f\"YOLOv8 best 모델 복사 완료: yolobest.pt\", bLines=False)\n",
        "\n",
        "            if os.path.exists(last_pt):\n",
        "                shutil.copy(last_pt, os.path.join(MODEL_FILES, \"yololast.pt\"))\n",
        "                OpLog(f\"YOLOv8 last 모델 복사 완료: yololast.pt\", bLines=False)\n",
        "        except Exception as e:\n",
        "            OpLog(f\"YOLOv8 모델 파일 복사 중 오류: {e}\", bLines=False)\n",
        "\n",
        "        # YOLOv8 학습 결과에서 각 에폭의 메트릭을 CSV에 저장\n",
        "        try:\n",
        "            # YOLOv8 results.csv 파일에서 메트릭 읽기\n",
        "            results_dir = os.path.join(data_root, \"yolo_results\", f\"yolov8{self.model_size}_train\")\n",
        "            results_csv = os.path.join(results_dir, \"results.csv\")\n",
        "\n",
        "            if os.path.exists(results_csv):\n",
        "                import pandas as pd\n",
        "                yolo_results = pd.read_csv(results_csv)\n",
        "\n",
        "                # 각 에폭의 메트릭을 RESULT_CSV에 저장\n",
        "                for idx, row in yolo_results.iterrows():\n",
        "                    epoch_num = idx + 1\n",
        "\n",
        "                    # YOLOv8 결과에서 메트릭 추출 (컬럼명은 YOLOv8 버전에 따라 다를 수 있음)\n",
        "                    train_loss = row.get('train/box_loss', 0) + row.get('train/cls_loss', 0) + row.get('train/dfl_loss', 0)\n",
        "                    val_loss = row.get('val/box_loss', 0) + row.get('val/cls_loss', 0) + row.get('val/dfl_loss', 0)\n",
        "                    mAP50 = row.get('metrics/mAP50(B)', None)\n",
        "                    mAP50_95 = row.get('metrics/mAP50-95(B)', None)\n",
        "                    precision = row.get('metrics/precision(B)', None)\n",
        "                    recall = row.get('metrics/recall(B)', None)\n",
        "\n",
        "                    # 학습 메트릭 저장\n",
        "                    self.save_metrics_to_csv(\n",
        "                        model_name=self.getMyName(),\n",
        "                        epoch_index=epoch_num,\n",
        "                        max_epochs=epochs,\n",
        "                        train_loss=train_loss if train_loss > 0 else None,\n",
        "                        current_lr=lr,\n",
        "                        val_loss=val_loss if val_loss > 0 else None,\n",
        "                        mAP50=mAP50,\n",
        "                        mAP50_95=mAP50_95,\n",
        "                        precision=precision,\n",
        "                        recall=recall,\n",
        "                        mode=\"train\",\n",
        "                    )\n",
        "\n",
        "                OpLog(f\"YOLOv8 학습 메트릭 {len(yolo_results)}개 에폭 저장 완료\", bLines=False)\n",
        "        except Exception as e:\n",
        "            OpLog(f\"YOLOv8 결과 파싱 중 오류: {e}\", bLines=False)\n",
        "\n",
        "        # 학습 완료 후 최종 검증 및 테스트 (1회만 수행)\n",
        "        if val_loader is not None:\n",
        "            OpLog(\"최종 검증 수행 중...\", bLines=True)\n",
        "            self.evalModel(yaml_file, val_loader, epochs, epochs)\n",
        "        if test_loader is not None:\n",
        "            OpLog(\"최종 테스트 수행 중...\", bLines=True)\n",
        "            self.testModel(test_img_dir, test_loader, epochs, epochs)\n",
        "\n",
        "        # 학습 결과 시각화\n",
        "        self.plot_results()\n",
        "        return results\n",
        "    def evalModel(self, yaml_file, val_loader, epoch, max_epochs):\n",
        "        \"\"\"\n",
        "        검증 데이터셋에 대한 모델 평가 (BaseModel 인터페이스 구현)\n",
        "        Args:\n",
        "            yaml_file: YAML 파일 경로\n",
        "            val_loader: 검증 데이터로더\n",
        "            epoch: 현재 에포크 번호\n",
        "            max_epochs: 전체 에포크 수\n",
        "        \"\"\"\n",
        "        OpLog(f\"[Epoch {epoch}/{max_epochs}] Validation 시작\", bLines=True)\n",
        "\n",
        "        # 모델 검증\n",
        "        metrics = self.model.val(\n",
        "            data=yaml_file,\n",
        "            device=DEVICE_TYPE,\n",
        "            split=\"val\",\n",
        "            plots=False,  # 매 에포크마다 플롯 생성 방지\n",
        "        )\n",
        "\n",
        "        # 주요 메트릭 추출\n",
        "        mAP50 = float(metrics.box.map50)\n",
        "        mAP50_95 = float(metrics.box.map)\n",
        "        precision = float(metrics.box.mp)\n",
        "        recall = float(metrics.box.mr)\n",
        "        val_loss = 1.0 - mAP50_95  # mAP를 손실로 변환 (높을수록 좋으므로 1에서 빼기)\n",
        "\n",
        "        # 검증 손실 저장\n",
        "        self.val_losses.append(val_loss)\n",
        "        OpLog(\n",
        "            f\"mAP50: {mAP50:.4f}, mAP50-95: {mAP50_95:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\",\n",
        "            bLines=False,\n",
        "        )\n",
        "\n",
        "        # 메트릭 딕셔너리 생성\n",
        "        metrics_dict = {\n",
        "            \"mAP50\": mAP50,\n",
        "            \"mAP50_95\": mAP50_95,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "        }\n",
        "\n",
        "        # 공통 헬퍼로 저장 및 시각화\n",
        "        self._save_eval_metrics(\n",
        "            epoch=epoch,\n",
        "            max_epochs=max_epochs,\n",
        "            metrics_dict=metrics_dict,\n",
        "            train_loss=0.0,\n",
        "            val_loss=val_loss,\n",
        "            current_lr=0.0,\n",
        "        )\n",
        "        return val_loss\n",
        "    def testModel(self, test_img_dir, test_loader, epoch, max_epochs):\n",
        "        \"\"\"\n",
        "        테스트 데이터셋에 대한 모델 평가 (BaseModel 인터페이스 구현)\n",
        "        Args:\n",
        "            test_img_dir: 테스트 이미지 디렉토리\n",
        "            test_loader: 테스트 데이터로더\n",
        "            epoch: 현재 에포크 번호\n",
        "            max_epochs: 전체 에포크 수\n",
        "        \"\"\"\n",
        "        OpLog(f\"[Epoch {epoch}/{max_epochs}] Test 시작\", bLines=True)\n",
        "\n",
        "        # 테스트 이미지에 대한 예측 수행\n",
        "        results = self.model.predict(\n",
        "            source=test_img_dir,\n",
        "            conf=0.25,\n",
        "            save=False,\n",
        "            device=DEVICE_TYPE,\n",
        "            verbose=False,\n",
        "        )\n",
        "\n",
        "        # 예측 결과 수집\n",
        "        predictions = []\n",
        "        for result in results:\n",
        "            pred_dict = {\n",
        "                \"boxes\": result.boxes.xyxy.cpu().numpy() if result.boxes else [],\n",
        "                \"scores\": result.boxes.conf.cpu().numpy() if result.boxes else [],\n",
        "                \"labels\": result.boxes.cls.cpu().numpy() if result.boxes else [],\n",
        "                \"filename\": os.path.basename(result.path),\n",
        "            }\n",
        "            predictions.append(pred_dict)\n",
        "        OpLog(f\"테스트 이미지 {len(predictions)}개 예측 완료\", bLines=False)\n",
        "\n",
        "        # 공통 헬퍼로 통계 계산, 저장 및 시각화\n",
        "        self._save_test_metrics(\n",
        "            epoch=epoch,\n",
        "            max_epochs=max_epochs,\n",
        "            predictions=predictions,\n",
        "            test_img_dir=test_img_dir,\n",
        "            train_loss=0.0,\n",
        "            current_lr=0.0,\n",
        "        )\n",
        "\n",
        "        # test_loss 계산 후 반환\n",
        "        total_detections = sum(len(p[\"boxes\"]) for p in predictions)\n",
        "        avg_conf = sum(\n",
        "            p[\"scores\"].mean() if len(p[\"scores\"]) > 0 else 0.0 for p in predictions\n",
        "        ) / max(len(predictions), 1)\n",
        "        test_loss = 1.0 - avg_conf\n",
        "\n",
        "        # Submission 파일 생성 (매 테스트마다 수행)\n",
        "        # 클래스 이름 가져오기 - yaml_file은 test_img_dir의 부모의 하위 디렉토리에 있음\n",
        "        # test_img_dir = data/oraldrug/test_images\n",
        "        # data_dir = data/oraldrug/1.drug_Image_annotation_allOK (또는 유사)\n",
        "        # yaml_file = data_dir/yolo_yaml.yaml\n",
        "\n",
        "        parent_dir = os.path.dirname(test_img_dir)  # data/oraldrug\n",
        "        yaml_file = None\n",
        "\n",
        "        # 부모 디렉토리의 하위 디렉토리에서 yolo_yaml.yaml 찾기\n",
        "        for item in os.listdir(parent_dir):\n",
        "            item_path = os.path.join(parent_dir, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                yaml_candidate = os.path.join(item_path, \"yolo_yaml.yaml\")\n",
        "                if os.path.exists(yaml_candidate):\n",
        "                    yaml_file = yaml_candidate\n",
        "                    break\n",
        "\n",
        "        if yaml_file and os.path.exists(yaml_file):\n",
        "            import yaml\n",
        "            with open(yaml_file, 'r', encoding='utf-8') as f:\n",
        "                yaml_data = yaml.safe_load(f)\n",
        "                class_names = yaml_data.get('names', [])\n",
        "            OpLog(f\"YAML 파일 로드: {yaml_file}\", bLines=False)\n",
        "        else:\n",
        "            OpLog(f\"Warning: yolo_yaml.yaml을 찾을 수 없습니다. 기본 클래스 이름 사용\", bLines=False)\n",
        "            class_names = list(range(self.num_classes)) if self.num_classes else []\n",
        "\n",
        "        self.CreateSubmission(\n",
        "            predictions=predictions,\n",
        "            test_img_dir=test_img_dir,\n",
        "            class_names=class_names,\n",
        "            save_image_num=10\n",
        "        )\n",
        "\n",
        "        return test_loss\n",
        "    def predict(self, source, conf=0.25, save=True):\n",
        "        \"\"\"\n",
        "        이미지에 대한 예측 수행\n",
        "        Args:\n",
        "            source: 이미지 경로, 폴더 경로, 또는 이미지 URL\n",
        "            conf: 신뢰도 임계값\n",
        "            save: 결과 저장 여부\n",
        "        \"\"\"\n",
        "        results = self.model.predict(\n",
        "            source=source,\n",
        "            conf=conf,\n",
        "            save=save,\n",
        "            project=os.path.join(BASE_DIR, \"yolo_results\"),\n",
        "            name=f\"yolov8{self.model_size}_predict\",\n",
        "            exist_ok=True,\n",
        "        )\n",
        "        return results\n",
        "    def load_yolo_model(self, model_path):\n",
        "        \"\"\"YOLOv8 모델 로드\"\"\"\n",
        "        if not os.path.exists(model_path):\n",
        "            OpLog(f\"모델 파일을 찾을 수 없습니다: {model_path}\", bLines=True)\n",
        "            return False\n",
        "        self.model = YOLO(model_path)\n",
        "        OpLog(f\"YOLOv8 모델 로드 완료: {model_path}\", bLines=True)\n",
        "        return True\n",
        "    def save_yolo_model(self, save_path=None):\n",
        "        \"\"\"YOLOv8 모델 저장\"\"\"\n",
        "        if save_path is None:\n",
        "            save_path = os.path.join(MODEL_FILES, f\"yolov8{self.model_size}_final.pt\")\n",
        "        makedirs(os.path.dirname(save_path))\n",
        "\n",
        "        # YOLOv8 모델 내보내기\n",
        "        self.model.export(format=\"torchscript\", dynamic=False)\n",
        "        OpLog(f\"YOLOv8 모델 저장됨: {save_path}\", bLines=True)\n",
        "        return save_path\n",
        "    def TestModelByBest(self, pt_file, test_img_dir, test_loader=None):\n",
        "        \"\"\"Best 모델 파일로 테스트 및 Submission 생성\n",
        "\n",
        "        Args:\n",
        "            pt_file: 로드할 .pt 모델 파일 경로\n",
        "            test_img_dir: 테스트 이미지 디렉토리\n",
        "            test_loader: 테스트 데이터 로더 (선택, 사용하지 않음)\n",
        "        \"\"\"\n",
        "        OpLog(f\"Best 모델로 테스트 시작: {pt_file}\", bLines=True)\n",
        "\n",
        "        # 모델 로드\n",
        "        if not self.load_yolo_model(pt_file):\n",
        "            OpLog(f\"모델 로드 실패: {pt_file}\", bLines=True)\n",
        "            return False\n",
        "\n",
        "        # testModel 호출 (epoch=1, max_epochs=1로 설정)\n",
        "        self.testModel(test_img_dir, test_loader, epoch=1, max_epochs=1)\n",
        "\n",
        "        OpLog(f\"Best 모델 테스트 및 Submission 생성 완료\", bLines=True)\n",
        "        return True\n",
        "    def plot_results(self):\n",
        "        \"\"\"학습 결과 시각화\"\"\"\n",
        "        results_dir = os.path.join(\n",
        "            BASE_DIR, \"yolo_results\", f\"yolov8{self.model_size}_train\"\n",
        "        )\n",
        "        results_file = os.path.join(results_dir, \"results.png\")\n",
        "        if os.path.exists(results_file):\n",
        "            OpLog(f\"학습 결과 그래프: {results_file}\", bLines=False)\n",
        "        else:\n",
        "            OpLog(\"학습 결과 파일을 찾을 수 없습니다.\", bLines=False)\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0lZAte3A4xl"
      },
      "source": [
        "════════════════════════════════════════<br>\n",
        "▣ 06. Faster R-CNN 모델 정의<br>\n",
        "════════════════════════════════════════"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "G_n4pM4gA4xl"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Pu19GEfYA4xl"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "class FasterRCNNModel(BaseModel):\n",
        "    def save_model(self, epoch_index, is_best=False, **kwargs):\n",
        "        \"\"\"현재 모델 상태를 저장 (FasterRCNN 전용 파일명)\n",
        "        Args:\n",
        "            epoch_index: 현재 에포크 번호\n",
        "            is_best: Best 모델인지 여부\n",
        "            **kwargs: 추가로 저장할 데이터 (model_state_dict, train_losses 등)\n",
        "        \"\"\"\n",
        "        save_dir = MODEL_FILES\n",
        "        makedirs(save_dir)\n",
        "\n",
        "        # FasterRCNN 전용 파일명\n",
        "        if is_best:\n",
        "            filename = os.path.join(save_dir, \"fasterbest.pt\")\n",
        "        else:\n",
        "            filename = os.path.join(save_dir, \"fasterlast.pt\")\n",
        "\n",
        "        # 기본 저장 데이터\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch_index,\n",
        "            \"is_best\": is_best,\n",
        "            \"model_name\": self.getMyName(),\n",
        "        }\n",
        "\n",
        "        # kwargs로 전달된 추가 데이터 저장\n",
        "        checkpoint.update(kwargs)\n",
        "        torch.save(checkpoint, filename)\n",
        "        if is_best:\n",
        "            print(f\"  Best 모델 저장됨: {filename}\")\n",
        "            OpLog(f\"Best model saved: {filename}\")\n",
        "        else:\n",
        "            OpLog(f\"모델 저장됨: {filename}\", bLines=False)\n",
        "    \"\"\"\n",
        "    Faster R-CNN 기반 객체 탐지 모델\n",
        "    - torchvision의 사전 학습된 Faster R-CNN 사용\n",
        "    - ResNet50-FPN 백본 활용\n",
        "    - 객체 탐지 및 분류 동시 수행\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, backbone=\"resnet50\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_classes: 클래스 수 (필수, +1은 배경 클래스)\n",
        "            backbone: 백본 네트워크 ('resnet50', 'mobilenet' 등)\n",
        "        \"\"\"\n",
        "        super().__init__() # Changed from super(FasterRCNNModel, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        self.num_classes = num_classes + 1  # +1 for background\n",
        "\n",
        "        # 사전 훈련된 Faster R-CNN 모델 로드\n",
        "        if backbone == \"resnet50\":\n",
        "            self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
        "                pretrained=True\n",
        "            )\n",
        "        elif backbone == \"mobilenet\":\n",
        "            self.model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(\n",
        "                pretrained=True\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
        "\n",
        "        # 분류 헤드를 클래스 수에 맞게 변경\n",
        "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
        "        self.model.roi_heads.box_predictor = (\n",
        "            torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
        "                in_features, self.num_classes\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # 학습 이력 저장용\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.best_val_loss = float(\"inf\")\n",
        "        OpLog(\n",
        "            f\"Faster R-CNN 모델 초기화 완료 (backbone: {backbone}, classes: {self.num_classes})\",\n",
        "            bLines=False,\n",
        "        )\n",
        "    def getMyName(self):\n",
        "        return f\"FasterRCNNModel_{self.backbone}\"\n",
        "    def getOptimizer(self, lr=0.001, gubun=\"freeze\"):\n",
        "        \"\"\"\n",
        "        Faster R-CNN 모델의 optimizer를 반환\n",
        "        Args:\n",
        "            lr: 학습률\n",
        "            gubun: 최적화 방식\n",
        "                - 'freeze': backbone을 고정하고 head만 학습\n",
        "                - 'partial': backbone은 낮은 lr, head는 일반 lr로 학습\n",
        "                - 'all': 전체 모델 학습\n",
        "        Returns:\n",
        "            optimizer: torch.optim.SGD optimizer\n",
        "        \"\"\"\n",
        "        if gubun == \"partial\":\n",
        "            # Backbone과 head를 다른 학습률로 설정\n",
        "            params = [\n",
        "                {\n",
        "                    \"params\": self.model.backbone.parameters(),\n",
        "                    \"lr\": lr * 0.1,\n",
        "                },  # backbone은 10% lr\n",
        "                {\n",
        "                    \"params\": self.model.roi_heads.parameters(),\n",
        "                    \"lr\": lr,\n",
        "                },  # head는 100% lr\n",
        "            ]\n",
        "            OpLog(\n",
        "                f\"Optimizer 생성: partial mode (backbone_lr={lr*0.1:.6f}, head_lr={lr:.6f})\",\n",
        "                bLines=False,\n",
        "            )\n",
        "        elif gubun == \"freeze\":\n",
        "            # Backbone을 고정하고 head만 학습\n",
        "            for param in self.model.backbone.parameters():\n",
        "                param.requires_grad = False\n",
        "            params = self.model.roi_heads.parameters()\n",
        "            OpLog(f\"Optimizer 생성: freeze mode (only head, lr={lr:.6f})\", bLines=False)\n",
        "        else:  # 'all' or default\n",
        "            # 전체 모델 학습\n",
        "            params = self.model.parameters()\n",
        "            OpLog(f\"Optimizer 생성: all mode (lr={lr:.6f})\", bLines=False)\n",
        "        optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "        return optimizer\n",
        "    def fit(\n",
        "        self,\n",
        "        test_img_dir,\n",
        "        gubun=\"freeze\",\n",
        "        train_loader=None,\n",
        "        val_loader=None,\n",
        "        test_loader=None,\n",
        "        epochs=50,\n",
        "        imgsz=640,\n",
        "        batch_size=16,\n",
        "        lr=0.005,\n",
        "        patience=10,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Faster R-CNN 모델 학습 (BaseModel 인터페이스 준수)\n",
        "        Args:\n",
        "            test_img_dir: 테스트 이미지 디렉토리\n",
        "            gubun: 최적화 방식 ('freeze', 'partial', 'all')\n",
        "            train_loader: 학습 데이터 로더\n",
        "            val_loader: 검증 데이터 로더 (선택)\n",
        "            test_loader: 테스트 데이터 로더 (선택)\n",
        "            epochs: 학습 에포크 수\n",
        "            imgsz: 입력 이미지 크기 (FasterRCNN은 사용하지 않음, 인터페이스 통일용)\n",
        "            batch_size: 배치 크기 (FasterRCNN은 사용하지 않음, 인터페이스 통일용)\n",
        "            lr: 학습률\n",
        "            patience: Early stopping patience (과적합 방지)\n",
        "        \"\"\"\n",
        "        OpLog(f\"Faster R-CNN 모델 학습 시작 (Epochs: {epochs}, LR: {lr}, Patience: {patience})\", bLines=True)\n",
        "        self.model.to(DEVICE_TYPE)\n",
        "\n",
        "        # 옵티마이저 및 스케줄러 설정\n",
        "        params = [p for p in self.model.parameters() if p.requires_grad]\n",
        "        self.optimizer = self.getOptimizer(lr=lr, gubun=\"partial\")\n",
        "        self.lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "            self.optimizer, step_size=3, gamma=0.1\n",
        "        )\n",
        "\n",
        "        # Early stopping 변수 초기화\n",
        "        patience_counter = 0\n",
        "        best_val_loss = float('inf')\n",
        "\n",
        "        # 학습 루프\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            epoch_loss = 0.0\n",
        "            batch_count = 0\n",
        "            for batch_idx, (images, targets) in enumerate(train_loader):\n",
        "                # 이미지를 디바이스로 이동\n",
        "                images = [img.to(DEVICE_TYPE) for img in images]\n",
        "                # 타겟 준비\n",
        "                if isinstance(targets, torch.Tensor):\n",
        "                    targets = [\n",
        "                        {\n",
        "                            \"boxes\": torch.tensor(\n",
        "                                [[0, 0, 224, 224]], dtype=torch.float32\n",
        "                            ).to(DEVICE_TYPE),\n",
        "                            \"labels\": torch.tensor(\n",
        "                                [label.item()], dtype=torch.int64\n",
        "                            ).to(DEVICE_TYPE),\n",
        "                        }\n",
        "                        for label in targets\n",
        "                    ]\n",
        "                else:\n",
        "                    targets = [\n",
        "                        {k: v.to(DEVICE_TYPE) for k, v in t.items()} for t in targets\n",
        "                    ]\n",
        "                # Forward pass\n",
        "                loss_dict = self.model(images, targets)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "                # Backward pass\n",
        "                self.optimizer.zero_grad()\n",
        "                losses.backward()\n",
        "                self.optimizer.step()\n",
        "                epoch_loss += losses.item()\n",
        "                batch_count += 1\n",
        "                msg =  f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {losses.item():.4f}\"\n",
        "                print(msg,end=\"\\r\")\n",
        "\n",
        "                if batch_idx % 10 == 0:\n",
        "                    OpLog(\n",
        "                        f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {losses.item():.4f}\",\n",
        "                        bLines=False,\n",
        "                    )\n",
        "\n",
        "            # 에포크 평균 손실\n",
        "            avg_train_loss = epoch_loss / batch_count\n",
        "            self.train_losses.append(avg_train_loss)\n",
        "\n",
        "            # 현재 학습률 가져오기\n",
        "            current_lr = self.optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "            # 매 epoch마다 학습 메트릭 저장\n",
        "            self.save_metrics_to_csv(\n",
        "                model_name=self.getMyName(),\n",
        "                epoch_index=epoch + 1,\n",
        "                max_epochs=epochs,\n",
        "                train_loss=avg_train_loss,\n",
        "                current_lr=current_lr,\n",
        "                mode=\"train\",\n",
        "            )\n",
        "\n",
        "            # 매 epoch 검증\n",
        "            if val_loader is not None:\n",
        "                self.evalModel(val_loader, epoch + 1, epochs)\n",
        "                # Best 모델 저장 및 early stopping 검사\n",
        "                current_val_loss = self.val_losses[-1]\n",
        "                if current_val_loss < best_val_loss:\n",
        "                    best_val_loss = current_val_loss\n",
        "                    self.best_val_loss = best_val_loss\n",
        "                    patience_counter = 0\n",
        "                    OpLog(f\"Best 모델 업데이트! Val Loss: {best_val_loss:.4f}\", bLines=False)\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    OpLog(f\"Patience counter: {patience_counter}/{patience}\", bLines=False)\n",
        "\n",
        "                    if patience_counter >= patience:\n",
        "                        OpLog(f\"Early stopping triggered! {patience} epochs without improvement\", bLines=True)\n",
        "                        break\n",
        "                    self.save_model(\n",
        "                        epoch_index=epoch + 1,\n",
        "                        is_best=True,\n",
        "                        model_state_dict=self.model.state_dict(),\n",
        "                        optimizer_state_dict=self.optimizer.state_dict(),\n",
        "                        train_loss=avg_train_loss,\n",
        "                        val_loss=self.val_losses[-1],\n",
        "                    )\n",
        "\n",
        "            # 10 epoch마다 테스트\n",
        "            if test_loader is not None and (epoch + 1) % 10 == 0:\n",
        "                self.testModel(test_img_dir, test_loader, epoch + 1, epochs)\n",
        "\n",
        "            # 매 epoch마다 모델 저장\n",
        "            self.save_model(\n",
        "                epoch_index=epoch + 1,\n",
        "                is_best=False,\n",
        "                model_state_dict=self.model.state_dict(),\n",
        "                optimizer_state_dict=self.optimizer.state_dict(),\n",
        "                train_loss=avg_train_loss,\n",
        "                val_loss=self.val_losses[-1] if val_loader else None,\n",
        "                test_loss=(\n",
        "                    self.test_losses[-1]\n",
        "                    if (test_loader and len(self.test_losses) > 0)\n",
        "                    else None\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            # 학습률 스케줄러 업데이트\n",
        "            self.lr_scheduler.step()\n",
        "\n",
        "        # 학습 완료 후 최종 테스트 (early stopping으로 중간에 종료되었을 수도 있으므로)\n",
        "        if test_loader is not None:\n",
        "            OpLog(\"최종 테스트 수행 중...\", bLines=True)\n",
        "            final_epoch = epoch + 1  # 실제 학습이 완료된 에포크\n",
        "            self.testModel(test_img_dir, test_loader, final_epoch, epochs)\n",
        "        OpLog(\"Faster R-CNN 학습 완료!\", bLines=True)\n",
        "        self.plot_results()\n",
        "    def evalModel(self, val_loader, epoch, max_epochs):\n",
        "        \"\"\"검증 모드\"\"\"\n",
        "        self.model.train()  # Faster R-CNN은 train 모드에서 loss 반환\n",
        "        val_loss = 0.0\n",
        "        batch_count = 0\n",
        "        predictions_all = []\n",
        "        with torch.no_grad():\n",
        "            for images, targets in val_loader:\n",
        "                images = [img.to(DEVICE_TYPE) for img in images]\n",
        "                # 타겟 준비\n",
        "                if isinstance(targets, torch.Tensor):\n",
        "                    targets = [\n",
        "                        {\n",
        "                            \"boxes\": torch.tensor(\n",
        "                                [[0, 0, 224, 224]], dtype=torch.float32\n",
        "                            ).to(DEVICE_TYPE),\n",
        "                            \"labels\": torch.tensor(\n",
        "                                [label.item()], dtype=torch.int64\n",
        "                            ).to(DEVICE_TYPE),\n",
        "                        }\n",
        "                        for label in targets\n",
        "                    ]\n",
        "                else:\n",
        "                    targets = [\n",
        "                        {k: v.to(DEVICE_TYPE) for k, v in t.items()} for t in targets\n",
        "                    ]\n",
        "                loss_dict = self.model(images, targets)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "                val_loss += losses.item()\n",
        "                batch_count += 1\n",
        "                # 예측 수집 (시각화용)\n",
        "                self.model.eval()\n",
        "                preds = self.model(images)\n",
        "                predictions_all.extend(preds)\n",
        "                self.model.train()\n",
        "        avg_val_loss = val_loss / batch_count if batch_count > 0 else 0.0\n",
        "        self.val_losses.append(avg_val_loss)\n",
        "\n",
        "        # 예측 통계 계산\n",
        "        total_detections = sum(len(p[\"boxes\"]) for p in predictions_all)\n",
        "        avg_confidence = sum(\n",
        "            p[\"scores\"].mean().item() if len(p[\"scores\"]) > 0 else 0.0\n",
        "            for p in predictions_all\n",
        "        ) / max(len(predictions_all), 1)\n",
        "\n",
        "        # 정밀도/재현율 근사 계산 (신뢰도 기반)\n",
        "        precision = avg_confidence if avg_confidence > 0 else None\n",
        "        recall = avg_confidence * 0.9 if avg_confidence > 0 else None  # 근사값\n",
        "        OpLog(\n",
        "            f\"Epoch [{epoch}/{max_epochs}] - Val Loss: {avg_val_loss:.4f}, Detections: {total_detections}, Avg Conf: {avg_confidence:.4f}\", bLines=True\n",
        "        )\n",
        "\n",
        "        # 메트릭 딕셔너리 생성\n",
        "        current_lr = self.optimizer.param_groups[0][\"lr\"]\n",
        "        metrics_dict = {\n",
        "            'mAP50': None,  # FasterRCNN은 mAP 계산 안 함\n",
        "            'mAP50_95': None,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'total_detections': total_detections,\n",
        "            'avg_confidence': avg_confidence,\n",
        "        }\n",
        "\n",
        "        # 공통 헬퍼로 저장 및 시각화\n",
        "        self._save_eval_metrics(\n",
        "            epoch=epoch,\n",
        "            max_epochs=max_epochs,\n",
        "            metrics_dict=metrics_dict,\n",
        "            train_loss=self.train_losses[-1] if len(self.train_losses) > 0 else None,\n",
        "            val_loss=avg_val_loss,\n",
        "            current_lr=current_lr,\n",
        "        )\n",
        "    def testModel(self, test_img_dir, test_loader, epoch, max_epochs):\n",
        "        \"\"\"테스트 모드\n",
        "\n",
        "        Args:\n",
        "            test_img_dir: 테스트 이미지 디렉토리\n",
        "            test_loader: 테스트 데이터 로더 (사용하지 않음, 인터페이스 통일용)\n",
        "            epoch: 현재 에포크\n",
        "            max_epochs: 최대 에포크\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        predictions_all = []\n",
        "\n",
        "        # test_img_dir에서 직접 이미지 파일 리스트 가져오기\n",
        "        if not os.path.exists(test_img_dir):\n",
        "            OpLog(f\"테스트 이미지 디렉토리가 없습니다: {test_img_dir}\", bLines=True)\n",
        "            return\n",
        "\n",
        "        image_files = [\n",
        "            f for f in os.listdir(test_img_dir)\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))\n",
        "        ]\n",
        "\n",
        "        if len(image_files) == 0:\n",
        "            OpLog(f\"테스트 이미지가 없습니다: {test_img_dir}\", bLines=True)\n",
        "            return\n",
        "\n",
        "        # 전처리 변환 (FasterRCNN 표준)\n",
        "        from torchvision import transforms\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        with torch.no_grad():\n",
        "            for img_file in image_files:\n",
        "                img_path = os.path.join(test_img_dir, img_file)\n",
        "\n",
        "                # 이미지 로드 및 전처리\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                img_tensor = transform(img).to(DEVICE_TYPE)\n",
        "\n",
        "                # 예측 (배치 형태로 전달)\n",
        "                preds = self.model([img_tensor])\n",
        "\n",
        "                # 예측 결과에 파일명 추가\n",
        "                pred_dict = {\n",
        "                    \"boxes\": preds[0][\"boxes\"].cpu().numpy(),\n",
        "                    \"scores\": preds[0][\"scores\"].cpu().numpy(),\n",
        "                    \"labels\": preds[0][\"labels\"].cpu().numpy(),\n",
        "                    \"filename\": img_file,\n",
        "                }\n",
        "                predictions_all.append(pred_dict)\n",
        "\n",
        "        # 공통 헬퍼로 통계 계산, 저장 및 시각화\n",
        "        current_lr = self.optimizer.param_groups[0][\"lr\"]\n",
        "        self._save_test_metrics(\n",
        "            epoch=epoch,\n",
        "            max_epochs=max_epochs,\n",
        "            predictions=predictions_all,\n",
        "            test_img_dir=test_img_dir,\n",
        "            train_loss=self.train_losses[-1] if len(self.train_losses) > 0 else None,\n",
        "            current_lr=current_lr,\n",
        "        )\n",
        "\n",
        "        # Submission 파일 생성 (최종 에포크일 때만)\n",
        "        # 클래스 이름 가져오기 (0-based index이므로 num_classes-1개)\n",
        "        class_names = list(range(self.num_classes - 1))  # -1은 배경 클래스 제외\n",
        "        self.CreateSubmission(\n",
        "            predictions=predictions_all,\n",
        "            test_img_dir=test_img_dir,\n",
        "            class_names=class_names,\n",
        "            save_image_num=10\n",
        "        )\n",
        "    def TestModelByBest(self, pt_file, test_img_dir, test_loader=None):\n",
        "        \"\"\"Best 모델 파일로 테스트 및 Submission 생성\n",
        "\n",
        "        Args:\n",
        "            pt_file: 로드할 .pt 모델 파일 경로\n",
        "            test_img_dir: 테스트 이미지 디렉토리\n",
        "            test_loader: 테스트 데이터 로더 (선택, 사용하지 않음)\n",
        "        \"\"\"\n",
        "        OpLog(f\"Best 모델로 테스트 시작: {pt_file}\", bLines=True)\n",
        "\n",
        "        # 모델 로드\n",
        "        checkpoint = self.load_model(pt_file)\n",
        "        if checkpoint is None:\n",
        "            OpLog(f\"모델 로드 실패: {pt_file}\", bLines=True)\n",
        "            return False\n",
        "\n",
        "        # 모델 state_dict 로드\n",
        "        if 'model_state_dict' in checkpoint:\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.model.to(DEVICE_TYPE)\n",
        "            OpLog(f\"FasterRCNN 모델 로드 완료: {pt_file}\", bLines=False)\n",
        "        else:\n",
        "            OpLog(f\"model_state_dict를 찾을 수 없습니다: {pt_file}\", bLines=True)\n",
        "            return False\n",
        "\n",
        "        # testModel 호출 (epoch=1, max_epochs=1로 설정)\n",
        "        self.testModel(test_img_dir, test_loader, epoch=1, max_epochs=1)\n",
        "\n",
        "        OpLog(f\"Best 모델 테스트 및 Submission 생성 완료\", bLines=True)\n",
        "        return True\n",
        "    def plot_results(self):\n",
        "        \"\"\"학습 결과 시각화\"\"\"\n",
        "        if len(self.train_losses) == 0:\n",
        "            OpLog(\"학습 이력이 없습니다.\", bLines=False)\n",
        "            return\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # 손실 그래프\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.train_losses, label=\"Train Loss\", marker=\"o\")\n",
        "        if len(self.val_losses) > 0:\n",
        "            plt.plot(self.val_losses, label=\"Val Loss\", marker=\"s\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Faster R-CNN Training/Validation Loss\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # 손실 통계\n",
        "        plt.subplot(1, 2, 2)\n",
        "        stats_text = f\"Training Statistics\\n\\n\"\n",
        "        stats_text += f\"Epochs: {len(self.train_losses)}\\n\"\n",
        "        stats_text += f\"Final Train Loss: {self.train_losses[-1]:.4f}\\n\"\n",
        "        if len(self.val_losses) > 0:\n",
        "            stats_text += f\"Final Val Loss: {self.val_losses[-1]:.4f}\\n\"\n",
        "            stats_text += f\"Best Val Loss: {self.best_val_loss:.4f}\\n\"\n",
        "        plt.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment=\"center\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # 그래프 저장\n",
        "        save_dir = os.path.join(BASE_DIR, \"oraldrug\", \"results\")\n",
        "        makedirs(save_dir)\n",
        "        save_path = os.path.join(save_dir, \"fasterrcnn_training_results.png\")\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
        "        OpLog(f\"학습 결과 그래프 저장: {save_path}\", bLines=False)\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqItxhroA4xm"
      },
      "source": [
        "════════════════════════════════════════<br>\n",
        "▣ 07. 모델 생성 및 학습 실행<br>\n",
        "════════════════════════════════════════"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "4gMBKL-cA4xm"
      },
      "outputs": [],
      "source": [
        "def MakeModel(model_type, num_classes, model_size=\"n\", backbone=\"resnet50\", bBestLoad=False):\n",
        "    \"\"\"\n",
        "    모델 생성 함수\n",
        "    Args:\n",
        "        model_type: 모델 유형 (\"yolov8\" 또는 \"faster\")\n",
        "        num_classes: 클래스 수\n",
        "        model_size: YOLOv8 모델 크기 ('n', 's', 'm', 'l', 'x'), 기본값 'n'\n",
        "        backbone: FasterRCNN 백본 ('resnet50', 'mobilenet'), 기본값 'resnet50'\n",
        "        bBestLoad: True이면 best 모델 파일 로드 (yolobest.pt 또는 fasterbest.pt), 기본값 False\n",
        "    Returns:\n",
        "        model: 생성된 모델 객체\n",
        "    \"\"\"\n",
        "    if model_type == \"faster\":\n",
        "        OpLog(f\"FasterRCNN 모델 생성 중... (backbone={backbone}, num_classes={num_classes})\", bLines=False)\n",
        "        model = FasterRCNNModel(num_classes=num_classes, backbone=backbone)\n",
        "\n",
        "        # bBestLoad가 True이면 best 모델 로드\n",
        "        if bBestLoad:\n",
        "            best_model_path = os.path.join(MODEL_FILES, \"fasterbest.pt\")\n",
        "            if os.path.exists(best_model_path):\n",
        "                OpLog(f\"Best FasterRCNN 모델 로드 중: {best_model_path}\", bLines=True)\n",
        "                model.load_model(best_model_path)\n",
        "            else:\n",
        "                OpLog(f\"Best 모델 파일이 없습니다: {best_model_path}. 새로운 모델로 시작합니다.\", bLines=True)\n",
        "\n",
        "        return model\n",
        "    elif model_type == \"yolov8\":\n",
        "        OpLog(f\"YOLOv8 모델 생성 중... (model_size={model_size}, num_classes={num_classes})\", bLines=False)\n",
        "        model = YOLOv8Model(model_size=model_size, num_classes=num_classes)\n",
        "\n",
        "        # bBestLoad가 True이면 best 모델 로드\n",
        "        if bBestLoad:\n",
        "            best_model_path = os.path.join(MODEL_FILES, \"yolobest.pt\")\n",
        "            if os.path.exists(best_model_path):\n",
        "                OpLog(f\"Best YOLOv8 모델 로드 중: {best_model_path}\", bLines=True)\n",
        "                model.load_model(best_model_path)\n",
        "            else:\n",
        "                OpLog(f\"Best 모델 파일이 없습니다: {best_model_path}. 새로운 모델로 시작합니다.\", bLines=True)\n",
        "\n",
        "        return model\n",
        "    else:\n",
        "        raise ValueError(f\"지원되지 않는 모델 타입: {model_type}. 'yolov8' 또는 'faster'를 사용하세요.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "SWZbg0p0A4xm"
      },
      "outputs": [],
      "source": [
        "def Execute_Train(model_type, data_dir, model_size=\"n\", backbone=\"resnet50\", epochs=50, batch_size=16, lr=0.001, bBestLoad=False, **kwargs):\n",
        "    \"\"\"\n",
        "    모델 생성 및 학습 실행 함수\n",
        "\n",
        "    Args:\n",
        "        model_type: 모델 유형 (\"yolov8\" 또는 \"faster\")\n",
        "        data_dir: 데이터 디렉토리 경로\n",
        "        model_size: YOLOv8 모델 크기 ('n', 's', 'm', 'l', 'x'), 기본값 'n'\n",
        "        backbone: FasterRCNN 백본 ('resnet50', 'mobilenet'), 기본값 'resnet50'\n",
        "        epochs: 학습 에포크 수, 기본값 50\n",
        "        batch_size: 배치 크기, 기본값 16\n",
        "        lr: 학습률, 기본값 0.001 (FasterRCNN은 자동으로 0.005 사용)\n",
        "        bBestLoad: True이면 best 모델 로드 (yolobest.pt/fasterbest.pt), 기본값 False\n",
        "        **kwargs: fit 메서드에 전달할 추가 파라미터\n",
        "            - imgsz: 이미지 크기 (기본값 640)\n",
        "            - patience: Early stopping patience (YOLOv8, 기본값 10)\n",
        "            - gubun: 최적화 방식 (FasterRCNN, 기본값 'partial')\n",
        "            - train_ratio: 학습/검증 분할 비율 (기본값 0.8)\n",
        "            - num_workers: 데이터 로더 워커 수 (기본값 2)\n",
        "            - transform_type: 데이터 증강 타입 (기본값 'A')\n",
        "    \"\"\"\n",
        "    # kwargs에서 공통 파라미터 추출\n",
        "    imgsz = kwargs.pop('imgsz', 640)\n",
        "    patience = kwargs.pop('patience', 10)\n",
        "    gubun = kwargs.pop('gubun', 'partial')\n",
        "    train_ratio = kwargs.pop('train_ratio', 0.8)\n",
        "    num_workers = kwargs.pop('num_workers', 2)\n",
        "    transform_type = kwargs.pop('transform_type', 'A')\n",
        "\n",
        "    # 경로 설정\n",
        "    image_dir, annotation_dir, yaml_file, yaml_label_dir, test_img_dir = GetConfig(data_dir)\n",
        "\n",
        "    # 클래스 수 계산\n",
        "    num_classes = count_classes(annotation_dir)\n",
        "    OpLog(f\"총 클래스 수: {num_classes}\", bLines=True)\n",
        "\n",
        "    # 모델 생성 (bBestLoad 전달)\n",
        "    model = MakeModel(model_type, num_classes=num_classes, model_size=model_size, backbone=backbone, bBestLoad=bBestLoad)\n",
        "\n",
        "    # 데이터 로더 생성\n",
        "    train_loader, val_loader, test_loader = GetLoaders(\n",
        "        annotation_dir, transform_type, image_dir, test_img_dir,\n",
        "        batch_size=batch_size, train_ratio=train_ratio, num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    # 모델 타입에 따라 fit 호출 방식 구분\n",
        "    if isinstance(model, YOLOv8Model):\n",
        "        # YOLOv8 모델 학습\n",
        "        model.fit(\n",
        "            annotation_dir=annotation_dir,\n",
        "            image_dir=image_dir,\n",
        "            yaml_file=yaml_file,\n",
        "            yaml_label_dir=yaml_label_dir,\n",
        "            test_img_dir=test_img_dir,\n",
        "            epochs=epochs,\n",
        "            imgsz=imgsz,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            test_loader=test_loader,\n",
        "            patience=patience,\n",
        "            **kwargs  # 추가 파라미터 전달\n",
        "        )\n",
        "    elif isinstance(model, FasterRCNNModel):\n",
        "        # Faster R-CNN 모델 학습\n",
        "        # FasterRCNN은 기본 lr이 더 높음\n",
        "        actual_lr = lr if lr > 0.001 else 0.005\n",
        "        model.fit(\n",
        "            test_img_dir,\n",
        "            gubun=gubun,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            test_loader=test_loader,\n",
        "            epochs=epochs,\n",
        "            imgsz=imgsz,\n",
        "            batch_size=batch_size,\n",
        "            lr=actual_lr,\n",
        "            **kwargs  # 추가 파라미터 전달\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"지원되지 않는 모델 타입: {type(model)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "fq0iZsCmA4xz"
      },
      "outputs": [],
      "source": [
        "def testbest():\n",
        "    data_dir = r\"D:\\01.project\\EntryPrj\\data\\oraldrug\\1.drug_Image_annotation_allOK\"\n",
        "    image_dir, annotation_dir, yaml_file, yaml_label_dir, test_img_dir = GetConfig(data_dir)\n",
        "    # 클래스 수 계산\n",
        "    num_classes = count_classes(annotation_dir)\n",
        "    OpLog(f\"총 클래스 수: {num_classes}\", bLines=True)\n",
        "\n",
        "    # 모델 생성 (bBestLoad 전달)\n",
        "    model = MakeModel(\"yolov8\", 74)\n",
        "    model.TestModelByBest(MODEL_FILES + \"/yolobest.pt\", test_img_dir,20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFtyaRgFA4xz"
      },
      "source": [
        "이 부분을 추가하여 직접 실행할 때만 testbest() 호출"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "════════════════════════════════════════<br>\n",
        "▣ 07. Faster R-CNN 모델 생성<br>\n",
        "════════════════════════════════════════"
      ],
      "metadata": {
        "id": "fKr2wVS-Xr55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Corrected data_dir to point to the root of the unzipped data\n",
        "data_dir = '/content/images' # 실제 학습 데이터 경로의 root\n",
        "\n",
        "#data_dir = r\"D:\\01.project\\EntryPrj\\data\\oraldrug\\1.drug_Image_annotation_allOK\"\n",
        "trans_type = [\"default\"]#, \"A\", \"B\"]\n",
        "for transform_type in trans_type:\n",
        "  Execute_Train(\n",
        "            model_type=\"faster\",           # 모델 타입: \"faster\" 또는 \"yolov8\"\n",
        "            data_dir=data_dir,              # 데이터 디렉토리 경로\n",
        "            backbone=\"resnet50\",            # FasterRCNN 백본: \"resnet50\" 또는 \"mobilenet\"\n",
        "            epochs=30,                      # 학습 에포크 수\n",
        "            batch_size=8,                   # 배치 크기\n",
        "            lr=0.005,                       # 학습률 (FasterRCNN 권장: 0.005)\n",
        "            bBestLoad=True,                # Best 모델 로드 여부 (fasterbest.pt)\n",
        "            imgsz=640,                      # 이미지 크기\n",
        "            patience=10,                    # Early stopping patience (에포크 수)\n",
        "            gubun=\"partial\",                # 최적화 방식: \"freeze\", \"partial\", \"all\"\n",
        "            train_ratio=0.8,                # 학습/검증 데이터 분할 비율\n",
        "            num_workers=4,                  # 데이터 로더 워커 수\n",
        "            transform_type=transform_type              # 데이터 증강 타입: \"default\", \"A\", \"B\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KjVxF_YQDbeF",
        "outputId": "55b5a8d2-be5f-4a99-878b-5d61ffadbbe9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "설정된 image_dir: /content/images/train_images\n",
            "설정된 test_img_dir: /content/images/test_images\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:10:36] 총 클래스 수: 64\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160M/160M [00:00<00:00, 243MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:10:38] Best 모델 파일이 없습니다: /content/images/modelfiles/fasterbest.pt. 새로운 모델로 시작합니다.\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:10:39] Faster R-CNN 모델 학습 시작 (Epochs: 30, LR: 0.005, Patience: 10)\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:14:09] Epoch [1/30] - Val Loss: 0.1534, Detections: 1001, Avg Conf: 0.1624\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:17:39] Epoch [2/30] - Val Loss: 0.1193, Detections: 1325, Avg Conf: 0.1604\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:21:09] Epoch [3/30] - Val Loss: 0.1030, Detections: 1765, Avg Conf: 0.1042\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:24:38] Epoch [4/30] - Val Loss: 0.0999, Detections: 1167, Avg Conf: 0.1425\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:28:08] Epoch [5/30] - Val Loss: 0.0987, Detections: 1096, Avg Conf: 0.1539\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:31:38] Epoch [6/30] - Val Loss: 0.0974, Detections: 1078, Avg Conf: 0.1544\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:35:07] Epoch [7/30] - Val Loss: 0.0971, Detections: 1087, Avg Conf: 0.1497\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:38:37] Epoch [8/30] - Val Loss: 0.0977, Detections: 1084, Avg Conf: 0.1493\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "  Best 모델 저장됨: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:38:37] Best model saved: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:42:07] Epoch [9/30] - Val Loss: 0.0973, Detections: 1081, Avg Conf: 0.1508\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "  Best 모델 저장됨: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:42:07] Best model saved: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:45:37] Epoch [10/30] - Val Loss: 0.0972, Detections: 1081, Avg Conf: 0.1510\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "  Best 모델 저장됨: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:45:38] Best model saved: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:46:31] Epoch [10/30] - Test: 657 images, 1503 detections, Avg Conf: 0.1402\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:46:33] Submission 생성 시작: /content/images/submission/submission20251215124633\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:46:47] Submission 생성 완료: /content/images/submission/submission20251215124633\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:50:17] Epoch [11/30] - Val Loss: 0.0972, Detections: 1081, Avg Conf: 0.1508\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best 모델 저장됨: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:50:17] Best model saved: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:53:47] Epoch [12/30] - Val Loss: 0.0972, Detections: 1081, Avg Conf: 0.1510\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best 모델 저장됨: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:53:48] Best model saved: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 12:57:17] Epoch [13/30] - Val Loss: 0.0970, Detections: 1081, Avg Conf: 0.1510\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:00:47] Epoch [14/30] - Val Loss: 0.0970, Detections: 1081, Avg Conf: 0.1509\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:04:17] Epoch [15/30] - Val Loss: 0.0972, Detections: 1081, Avg Conf: 0.1509\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best 모델 저장됨: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:04:17] Best model saved: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:07:47] Epoch [16/30] - Val Loss: 0.0973, Detections: 1081, Avg Conf: 0.1509\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best 모델 저장됨: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:07:48] Best model saved: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:11:18] Epoch [17/30] - Val Loss: 0.0972, Detections: 1081, Avg Conf: 0.1510\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best 모델 저장됨: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:11:18] Best model saved: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:14:48] Epoch [18/30] - Val Loss: 0.0974, Detections: 1081, Avg Conf: 0.1510\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best 모델 저장됨: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:14:49] Best model saved: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:18:18] Epoch [19/30] - Val Loss: 0.0971, Detections: 1081, Avg Conf: 0.1510\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best 모델 저장됨: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:18:19] Best model saved: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:21:49] Epoch [20/30] - Val Loss: 0.0972, Detections: 1081, Avg Conf: 0.1510\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best 모델 저장됨: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:21:49] Best model saved: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:22:45] Epoch [20/30] - Test: 657 images, 1497 detections, Avg Conf: 0.1406\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:22:47] Submission 생성 시작: /content/images/submission/submission20251215132247\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:23:01] Submission 생성 완료: /content/images/submission/submission20251215132247\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:26:31] Epoch [21/30] - Val Loss: 0.0971, Detections: 1081, Avg Conf: 0.1510\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Best 모델 저장됨: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 13:26:31] Best model saved: /content/images/modelfiles/fasterbest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2232862885.py:126: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4057624619.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrans_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#, \"A\", \"B\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtransform_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrans_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   Execute_Train(\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"faster\"\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0;31m# 모델 타입: \"faster\" 또는 \"yolov8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m              \u001b[0;31m# 데이터 디렉토리 경로\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1438341721.py\u001b[0m in \u001b[0;36mExecute_Train\u001b[0;34m(model_type, data_dir, model_size, backbone, epochs, batch_size, lr, bBestLoad, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# FasterRCNN은 기본 lr이 더 높음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mactual_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         model.fit(\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mtest_img_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mgubun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgubun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3123201726.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, test_img_dir, gubun, train_loader, val_loader, test_loader, epochs, imgsz, batch_size, lr, patience)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;31m# 매 epoch 검증\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevalModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# Best 모델 저장 및 early stopping 검사\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mcurrent_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3123201726.py\u001b[0m in \u001b[0;36mevalModel\u001b[0;34m(self, val_loader, epoch, max_epochs)\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;31m# 예측 수집 (시각화용)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m                 \u001b[0mpredictions_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         detections = self.transform.postprocess(\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         )  # type: ignore[operator]\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mpostprocess\u001b[0;34m(self, result, image_shapes, original_image_sizes)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_im_s\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_im_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"masks\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mresize_boxes\u001b[0;34m(boxes, original_size, new_size)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0mymin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mymin\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mratio_height\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0mymax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mymax\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mratio_height\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mymax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "d867f6c8",
        "outputId": "9b2577f4-bb8f-4c5a-adff-53b2cf264205"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming SUBMISSTION_DIR is correctly set to /content/drive/MyDrive/oraldrug/submission\n",
        "# You need to find the latest submission directory from the previous run.\n",
        "# From the logs, the last submission was: /content/drive/MyDrive/oraldrug/submission/submission20251215132247\n",
        "# Let's use that specific path for demonstration, or find the latest one programmatically.\n",
        "\n",
        "# Replace with the actual latest submission directory path if different\n",
        "submission_folder_path = '/content/images/submission/submission20251215132247'\n",
        "submission_csv_path = os.path.join(submission_folder_path, 'submission20251215132247.csv')\n",
        "\n",
        "if os.path.exists(submission_csv_path):\n",
        "    submission_df = pd.read_csv(submission_csv_path)\n",
        "    print(f\"Submission CSV 파일 로드 완료: {submission_csv_path}\")\n",
        "\n",
        "    # image_id 205에 해당하는 예측만 필터링\n",
        "    image_205_predictions = submission_df[submission_df['image_id'] == 205]\n",
        "\n",
        "    if not image_205_predictions.empty:\n",
        "        print(\"\\nImage ID 205에 대한 예측 결과:\")\n",
        "        display(image_205_predictions)\n",
        "    else:\n",
        "        print(\"\\nImage ID 205에 대한 예측 결과가 CSV에 없습니다. 모델이 아무것도 감지하지 못했습니다.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Submission CSV 파일을 찾을 수 없습니다: {submission_csv_path}\")\n",
        "    print(\"최근 Submission 폴더 경로를 확인하거나, 'submission_folder_path' 변수를 올바르게 설정해주세요.\")\n",
        "\n",
        "\n",
        "print(\"\\n가능한 원인:\")\n",
        "print(\"1. 모델이 약물을 아예 감지하지 못했습니다. (CSV에 해당 image_id 없음)\")\n",
        "print(\"2. 감지는 했지만 신뢰도(score)가 너무 낮아서 시각화 시 제외되었을 수 있습니다. (CSV에 낮은 score 기록)\")\n",
        "print(\"3. 바운딩 박스(bbox)가 너무 부정확하게 예측되었을 수 있습니다. (CSV의 bbox 확인)\")\n",
        "print(\"4. 해당 약물의 학습 데이터가 부족하거나, 이미지 품질이 좋지 않을 수 있습니다.\")\n",
        "print(\"5. 학습 시 설정한 'conf' 임계값이 너무 높을 수 있습니다.\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission CSV 파일 로드 완료: /content/images/submission/submission20251215132247/submission20251215132247.csv\n",
            "\n",
            "Image ID 205에 대한 예측 결과:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   annotation_id  image_id  category_id  bbox_x  bbox_y  bbox_w  bbox_h  score\n",
              "2              3       205            2      12       0     963    1280   0.21\n",
              "3              4       205            3      25     410     948     869   0.08"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71ae183e-fd06-4d7b-9913-649d5ff156b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>annotation_id</th>\n",
              "      <th>image_id</th>\n",
              "      <th>category_id</th>\n",
              "      <th>bbox_x</th>\n",
              "      <th>bbox_y</th>\n",
              "      <th>bbox_w</th>\n",
              "      <th>bbox_h</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>205</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>963</td>\n",
              "      <td>1280</td>\n",
              "      <td>0.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>205</td>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "      <td>410</td>\n",
              "      <td>948</td>\n",
              "      <td>869</td>\n",
              "      <td>0.08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71ae183e-fd06-4d7b-9913-649d5ff156b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-71ae183e-fd06-4d7b-9913-649d5ff156b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-71ae183e-fd06-4d7b-9913-649d5ff156b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0eacfb76-affc-4ca3-bec7-0cf498215425\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0eacfb76-affc-4ca3-bec7-0cf498215425')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0eacfb76-affc-4ca3-bec7-0cf498215425 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_dddea30b-bde4-4ee3-a8fc-eaf5b643c4ea\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('image_205_predictions')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dddea30b-bde4-4ee3-a8fc-eaf5b643c4ea button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('image_205_predictions');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "image_205_predictions",
              "summary": "{\n  \"name\": \"image_205_predictions\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"annotation_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 4,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 205,\n        \"max\": 205,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          205\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bbox_x\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9,\n        \"min\": 12,\n        \"max\": 25,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bbox_y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 289,\n        \"min\": 0,\n        \"max\": 410,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          410\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bbox_w\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10,\n        \"min\": 948,\n        \"max\": 963,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          948\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bbox_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 290,\n        \"min\": 869,\n        \"max\": 1280,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          869\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09192388155425117,\n        \"min\": 0.08,\n        \"max\": 0.21,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.08\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "가능한 원인:\n",
            "1. 모델이 약물을 아예 감지하지 못했습니다. (CSV에 해당 image_id 없음)\n",
            "2. 감지는 했지만 신뢰도(score)가 너무 낮아서 시각화 시 제외되었을 수 있습니다. (CSV에 낮은 score 기록)\n",
            "3. 바운딩 박스(bbox)가 너무 부정확하게 예측되었을 수 있습니다. (CSV의 bbox 확인)\n",
            "4. 해당 약물의 학습 데이터가 부족하거나, 이미지 품질이 좋지 않을 수 있습니다.\n",
            "5. 학습 시 설정한 'conf' 임계값이 너무 높을 수 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "════════════════════════════════════════<br>\n",
        "▣ 08. YOLO 모델 생성<br>\n",
        "════════════════════════════════════════"
      ],
      "metadata": {
        "id": "OCRDM7l1X34k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Corrected data_dir to point to the root of the unzipped data\n",
        "data_dir = '/content/images' # 실제 학습 데이터 경로의 root\n",
        "\n",
        "#data_dir = r\"D:\\01.project\\EntryPrj\\data\\oraldrug\\1.drug_Image_annotation_allOK\"\n",
        "trans_type = [\"default\"]#, \"A\", \"B\"]\n",
        "for transform_type in trans_type:\n",
        "  Execute_Train(\n",
        "            model_type=\"yolov8\",            # 모델 타입: \"yolov8\" 또는 \"faster\"\n",
        "            data_dir=data_dir,              # 데이터 디렉토리 경로\n",
        "            model_size=\"n\",                 # YOLOv8 모델 크기: \"n\", \"s\", \"m\", \"l\", \"x\"\n",
        "            epochs=50,                      # 학습 에포크 수\n",
        "            batch_size=16,                  # 배치 크기\n",
        "            lr=0.001,                       # 학습률 (YOLOv8 권장: 0.001)\n",
        "            bBestLoad=True,                # Best 모델 로드 여부 (yolobest.pt)\n",
        "            imgsz=640,                      # 이미지 크기\n",
        "            patience=10,                    # Early stopping patience (에포크 수)\n",
        "            train_ratio=0.8,                # 학습/검증 데이터 분할 비율\n",
        "            num_workers=4,         # 데이터 로더 워커 수\n",
        "            transform_type=transform_type   # 데이터 증강 타입: \"default\", \"A\", \"B\"\n",
        "        )"
      ],
      "metadata": {
        "id": "yf61WBQVDkZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d0a7ea-d593-42ae-824b-6715ac6de157"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 04:03:31] 총 클래스 수: 74\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 04:03:31] Best YOLOv8 모델 로드 중: /content/drive/MyDrive/oraldrug/4.drug_Augmentation/modelfiles/yolobest.pt\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 04:03:31] 모델 로드 완료: /content/drive/MyDrive/oraldrug/4.drug_Augmentation/modelfiles/yolobest.pt (Epoch -1)\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 04:04:06] YOLOv8n 모델 학습 시작\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 04:04:06] YAML 파일 및 레이블 재생성 중...\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 04:04:06] 기존 YAML 파일이 있으므로 preJob종료: /content/drive/MyDrive/oraldrug/4.drug_Augmentation/yolo_yaml.yaml\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/oraldrug/4.drug_Augmentation/yolo_yaml.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/oraldrug/4.drug_Augmentation/yolo_results, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/oraldrug/4.drug_Augmentation/yolo_results/yolov8n_train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=74\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    845662  ultralytics.nn.modules.head.Detect           [74, [64, 128, 256]]          \n",
            "Model summary: 129 layers, 3,105,198 parameters, 3,105,182 gradients, 8.6 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 1.2±1.7 ms, read: 301.8±119.9 MB/s, size: 1766.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/oraldrug/4.drug_Augmentation/train_images.cache... 786 images, 139 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 925/925 1.3Mit/s 0.0s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 1.0±1.3 ms, read: 127.1±24.7 MB/s, size: 1735.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/oraldrug/4.drug_Augmentation/train_images.cache... 786 images, 139 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 925/925 292.2Kit/s 0.0s\n",
            "Plotting labels to /content/drive/MyDrive/oraldrug/4.drug_Augmentation/yolo_results/yolov8n_train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000128, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/drive/MyDrive/oraldrug/4.drug_Augmentation/yolo_results/yolov8n_train\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/50      2.33G     0.7913      4.654      1.018         73        640: 100% ━━━━━━━━━━━━ 58/58 1.1it/s 52.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.7s\n",
            "                   all        925       2756     0.0101    0.00313    0.00673    0.00626\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/50       2.6G     0.5322        3.8     0.9203         60        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.8s\n",
            "                   all        925       2756       0.75      0.063     0.0967     0.0912\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/50       2.6G     0.4854       3.03     0.9144         72        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.7s\n",
            "                   all        925       2756      0.553      0.245      0.248      0.235\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/50       2.6G     0.4815      2.513      0.922         66        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.7s\n",
            "                   all        925       2756      0.706      0.381      0.455      0.437\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/50       2.6G     0.4551      2.031      0.912         67        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.0s\n",
            "                   all        925       2756      0.628      0.568      0.582      0.561\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/50       2.6G     0.4314      1.757     0.9055         71        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.4s\n",
            "                   all        925       2756      0.687      0.653      0.664       0.64\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/50       2.6G     0.4165      1.538     0.8935         61        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.1s/it 31.2s\n",
            "                   all        925       2756      0.721      0.704      0.747      0.718\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/50       2.6G     0.3913      1.345     0.8779         61        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.4s\n",
            "                   all        925       2756      0.817       0.72      0.776       0.75\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/50      2.62G     0.3895      1.266     0.8811         74        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.0s\n",
            "                   all        925       2756      0.801      0.792      0.803      0.778\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/50      2.64G     0.3765      1.158     0.8712         39        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.2s\n",
            "                   all        925       2756      0.827      0.818      0.833      0.811\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/50      2.65G     0.3685       1.08     0.8641         92        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.5s\n",
            "                   all        925       2756      0.863      0.827      0.856      0.834\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/50      2.67G     0.3625      1.056     0.8607         91        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.6s\n",
            "                   all        925       2756      0.851      0.865      0.878      0.858\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/50      2.67G     0.3615     0.9923     0.8617         76        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.3s\n",
            "                   all        925       2756      0.895      0.849      0.897      0.871\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/50      2.67G     0.3601     0.9381     0.8595         77        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.0s\n",
            "                   all        925       2756      0.866      0.884      0.906      0.883\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/50      2.67G     0.3449     0.9041     0.8581         81        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.3s\n",
            "                   all        925       2756      0.885       0.92      0.925      0.903\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/50      2.67G     0.3377     0.8762     0.8489         61        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 46.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.7s\n",
            "                   all        925       2756      0.885       0.93      0.929      0.906\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/50      2.67G     0.3429     0.8629     0.8579         71        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.1s\n",
            "                   all        925       2756      0.881       0.93      0.939      0.917\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/50      2.67G     0.3357     0.8281      0.852         88        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.7s\n",
            "                   all        925       2756      0.888       0.94      0.943      0.921\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/50      2.67G     0.3316     0.8079     0.8521         78        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.1s/it 31.1s\n",
            "                   all        925       2756      0.898      0.951      0.948      0.928\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/50      2.67G     0.3269     0.7913     0.8515         68        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.9s\n",
            "                   all        925       2756      0.911      0.951      0.951      0.934\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/50      2.67G     0.3269     0.7739     0.8503         75        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.7s\n",
            "                   all        925       2756       0.91      0.938      0.959       0.94\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/50      2.67G     0.3234      0.787     0.8487         69        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 46.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.3s\n",
            "                   all        925       2756      0.922       0.96      0.956      0.941\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/50      2.67G     0.3207     0.7455     0.8443         64        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.1s\n",
            "                   all        925       2756       0.92      0.953      0.953      0.938\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/50      2.67G     0.3191     0.7173     0.8472         63        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.3s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0it/s 28.8s\n",
            "                   all        925       2756      0.928      0.958      0.956       0.94\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/50      2.67G     0.3203     0.7146     0.8454         84        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 46.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.4s\n",
            "                   all        925       2756      0.944      0.946      0.964      0.945\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      26/50      2.67G     0.3131     0.7106     0.8478         65        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.2s\n",
            "                   all        925       2756      0.932      0.961      0.962      0.946\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      27/50      2.67G     0.3105     0.6956     0.8461         57        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.2s\n",
            "                   all        925       2756      0.941      0.955      0.968      0.955\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      28/50      2.67G     0.3084     0.6916     0.8436         63        640: 100% ━━━━━━━━━━━━ 58/58 1.2it/s 48.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.1s\n",
            "                   all        925       2756      0.932      0.955      0.979      0.963\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      29/50      2.67G      0.304     0.6645     0.8452         73        640: 100% ━━━━━━━━━━━━ 58/58 1.2it/s 47.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.2s\n",
            "                   all        925       2756      0.935      0.961      0.978      0.965\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      30/50      2.67G     0.3132     0.7001     0.8485         70        640: 100% ━━━━━━━━━━━━ 58/58 1.2it/s 46.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.1s/it 31.0s\n",
            "                   all        925       2756       0.93      0.974      0.977      0.964\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      31/50      2.67G     0.2989     0.6613     0.8397         74        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.4s\n",
            "                   all        925       2756      0.934      0.975      0.984       0.97\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      32/50      2.67G     0.3038     0.6728     0.8404         72        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.1s\n",
            "                   all        925       2756      0.945      0.967      0.984      0.971\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      33/50      2.67G     0.2983     0.6567     0.8447         58        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.3s\n",
            "                   all        925       2756      0.937      0.973      0.985      0.971\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      34/50      2.67G      0.296     0.6395     0.8412         51        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.8s\n",
            "                   all        925       2756      0.952      0.957      0.983      0.969\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      35/50      2.67G     0.2958     0.6487       0.84         60        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.4s\n",
            "                   all        925       2756      0.936       0.97      0.984      0.973\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      36/50      2.67G     0.2969     0.6431     0.8392         60        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.1s\n",
            "                   all        925       2756      0.956      0.956      0.983      0.972\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      37/50      2.67G     0.2936     0.6254     0.8412         78        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.0s\n",
            "                   all        925       2756      0.931      0.975      0.984      0.975\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      38/50      2.67G     0.2905     0.6091     0.8378         83        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0it/s 29.0s\n",
            "                   all        925       2756      0.946      0.972      0.986      0.976\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      39/50      2.67G     0.2897      0.617     0.8365         57        640: 100% ━━━━━━━━━━━━ 58/58 1.2it/s 47.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.8s\n",
            "                   all        925       2756      0.943      0.974      0.986      0.974\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      40/50      2.67G     0.2856      0.616     0.8389         73        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 30.1s\n",
            "                   all        925       2756      0.944       0.97      0.985      0.974\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      41/50      2.67G     0.2445     0.6772     0.8075         36        640: 100% ━━━━━━━━━━━━ 58/58 1.1it/s 51.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.1s\n",
            "                   all        925       2756      0.948      0.971      0.986      0.973\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      42/50      2.67G     0.2363     0.6338     0.8106         34        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.8s\n",
            "                   all        925       2756      0.929      0.987      0.986      0.973\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      43/50      2.67G     0.2282     0.6112     0.8007         41        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 43.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.9s\n",
            "                   all        925       2756      0.928       0.99      0.986      0.976\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      44/50      2.67G     0.2247     0.6059     0.8053         39        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.2s\n",
            "                   all        925       2756      0.955      0.969      0.986      0.976\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      45/50      2.67G     0.2243     0.6015     0.8052         38        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.4s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.6s\n",
            "                   all        925       2756      0.933       0.99      0.988      0.977\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      46/50      2.67G     0.2219     0.5901     0.7988         31        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.1s\n",
            "                   all        925       2756      0.944      0.987      0.988      0.978\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      47/50      2.67G     0.2218     0.5911     0.8007         41        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.2s\n",
            "                   all        925       2756      0.941      0.988      0.988      0.978\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      48/50      2.67G     0.2209     0.5853     0.8004         39        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 45.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.2s\n",
            "                   all        925       2756      0.933       0.99      0.987      0.978\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      49/50      2.67G     0.2173     0.5786     0.7976         45        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 43.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.2s\n",
            "                   all        925       2756      0.934      0.991      0.988      0.979\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      50/50      2.67G     0.2174     0.5823     0.7981         45        640: 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.0s/it 29.3s\n",
            "                   all        925       2756      0.935      0.991      0.988      0.978\n",
            "\n",
            "50 epochs completed in 1.060 hours.\n",
            "Optimizer stripped from /content/drive/MyDrive/oraldrug/4.drug_Augmentation/yolo_results/yolov8n_train/weights/last.pt, 6.4MB\n",
            "Optimizer stripped from /content/drive/MyDrive/oraldrug/4.drug_Augmentation/yolo_results/yolov8n_train/weights/best.pt, 6.4MB\n",
            "\n",
            "Validating /content/drive/MyDrive/oraldrug/4.drug_Augmentation/yolo_results/yolov8n_train/weights/best.pt...\n",
            "Ultralytics 8.3.237 🚀 Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 72 layers, 3,099,938 parameters, 0 gradients, 8.5 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 29/29 1.2s/it 33.7s\n",
            "                   all        925       2756      0.934      0.991      0.988      0.979\n",
            "                  1899        100        100      0.993          1      0.995      0.993\n",
            "                  2482         86         86      0.971          1      0.982      0.938\n",
            "                  3350        278        278      0.944          1      0.982      0.978\n",
            "                  3482        255        255      0.997          1      0.995      0.993\n",
            "                  3543         50         50      0.969          1      0.995      0.995\n",
            "                  3742         27         27      0.964          1      0.995      0.995\n",
            "                  3831         29         29      0.949          1      0.993       0.99\n",
            "                  4377         18         18      0.896          1      0.995      0.989\n",
            "                  4542         18         18      0.848          1      0.987      0.966\n",
            "                  5093         23         23      0.975          1      0.995      0.988\n",
            "                  5885         15         15      0.815          1      0.965      0.965\n",
            "                  6191         14         14      0.951          1      0.995      0.925\n",
            "                  6562         18         18      0.968          1      0.995      0.995\n",
            "                 10220         18         18      0.968          1      0.995      0.968\n",
            "                 10223          3          3          1      0.554      0.995      0.951\n",
            "                 12080         17         17      0.912          1      0.992      0.969\n",
            "                 12246         10         10      0.873          1      0.995      0.985\n",
            "                 12419          4          4      0.861          1      0.995      0.995\n",
            "                 12777         19         19      0.842          1      0.985      0.953\n",
            "                 13394         17         17      0.929          1      0.992       0.98\n",
            "                 13899         28         28      0.946          1      0.968      0.949\n",
            "                 16231         65         65      0.975          1      0.995      0.995\n",
            "                 16261         72         72      0.992          1      0.995      0.994\n",
            "                 16547         83         83      0.981          1      0.986      0.986\n",
            "                 16550         67         67      0.992          1      0.995      0.989\n",
            "                 16687         14         14      0.687      0.857      0.896      0.867\n",
            "                 18109         16         16      0.965          1      0.995      0.995\n",
            "                 18146         25         25      0.867          1      0.987      0.978\n",
            "                 18356         20         20      0.682          1      0.886      0.886\n",
            "                 19231         24         24      0.821          1      0.981      0.977\n",
            "                 19551         20         20      0.972          1      0.995      0.979\n",
            "                 19606         20         20       0.91          1      0.995      0.995\n",
            "                 19860         38         38      0.982          1      0.995      0.995\n",
            "                 20013         18         18      0.702      0.944      0.919      0.919\n",
            "                 20237         67         67      0.963          1      0.994      0.993\n",
            "                 20876         44         44      0.986          1      0.995      0.995\n",
            "                 21025         17         17      0.967          1      0.995      0.995\n",
            "                 21324         27         27      0.899          1      0.979      0.963\n",
            "                 21770         15         15      0.964          1      0.995      0.995\n",
            "                 22073         29         29      0.954          1      0.994      0.989\n",
            "                 22346         45         45      0.987          1      0.995      0.988\n",
            "                 22361         20         20      0.971          1      0.995      0.995\n",
            "                 22626         17         17      0.966          1      0.995      0.987\n",
            "                 23202          6          6      0.913          1      0.995      0.952\n",
            "                 23222         15         15      0.945          1      0.995      0.986\n",
            "                 24849         13         13      0.952          1      0.995      0.985\n",
            "                 25366         50         50      0.987          1      0.995      0.995\n",
            "                 25437         38         38      0.985          1      0.995      0.995\n",
            "                 25468         41         41      0.986          1      0.995      0.995\n",
            "                 27652         35         35      0.984          1      0.995      0.995\n",
            "                 27732         54         54      0.953      0.981      0.987      0.987\n",
            "                 27776         42         42      0.986          1      0.995      0.995\n",
            "                 27925         23         23      0.973          1      0.995      0.995\n",
            "                 27992          6          6       0.92          1      0.995      0.995\n",
            "                 28762         44         44      0.982          1      0.995      0.995\n",
            "                 29344         21         21      0.973          1      0.995       0.98\n",
            "                 29450         16         16      0.962          1      0.995      0.995\n",
            "                 29666         71         71      0.965          1      0.988      0.988\n",
            "                 29870          3          3      0.897          1      0.995      0.995\n",
            "                 30307         32         32      0.982          1      0.995      0.989\n",
            "                 31704         17         17      0.968          1      0.995      0.995\n",
            "                 31862         28         28      0.916          1      0.987      0.987\n",
            "                 31884         42         42       0.96      0.976      0.992       0.99\n",
            "                 32309         27         27      0.911          1      0.991      0.986\n",
            "                 33008         20         20      0.973          1      0.995      0.995\n",
            "                 33207         15         15      0.958          1      0.995      0.995\n",
            "                 33877          4          4      0.854          1      0.995      0.937\n",
            "                 33879         22         22      0.847          1      0.958      0.953\n",
            "                 34596         41         41      0.967          1      0.995      0.991\n",
            "                 35205         81         81      0.946      0.988      0.985      0.976\n",
            "                 36636         66         66      0.947          1      0.992      0.992\n",
            "                 38161         27         27      0.862          1      0.993      0.973\n",
            "                 41767         28         28      0.938          1      0.992      0.977\n",
            "                 44198         18         18      0.969          1      0.995      0.975\n",
            "Speed: 0.2ms preprocess, 2.2ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/oraldrug/4.drug_Augmentation/yolo_results/yolov8n_train\u001b[0m\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 05:08:42] YOLOv8 학습 완료!\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 05:08:44] 최종 검증 수행 중...\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 05:08:44] [Epoch 50/50] Validation 시작\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "Model summary (fused): 72 layers, 3,099,938 parameters, 0 gradients, 8.5 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.3±0.1 ms, read: 285.7±64.0 MB/s, size: 1757.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/oraldrug/4.drug_Augmentation/train_images.cache... 786 images, 139 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 925/925 1.2Mit/s 0.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 58/58 1.3it/s 44.6s\n",
            "                   all        925       2756      0.934      0.991      0.988      0.979\n",
            "                  1899        100        100      0.993          1      0.995      0.994\n",
            "                  2482         86         86      0.971          1      0.982      0.938\n",
            "                  3350        278        278      0.944          1      0.982      0.979\n",
            "                  3482        255        255      0.997          1      0.995      0.994\n",
            "                  3543         50         50      0.969          1      0.995      0.995\n",
            "                  3742         27         27      0.964          1      0.995      0.995\n",
            "                  3831         29         29      0.949          1      0.993       0.99\n",
            "                  4377         18         18      0.898          1      0.995      0.989\n",
            "                  4542         18         18      0.847          1      0.987      0.966\n",
            "                  5093         23         23      0.975          1      0.995      0.988\n",
            "                  5885         15         15      0.814          1      0.965      0.965\n",
            "                  6191         14         14      0.951          1      0.995      0.925\n",
            "                  6562         18         18      0.968          1      0.995      0.995\n",
            "                 10220         18         18      0.968          1      0.995      0.968\n",
            "                 10223          3          3          1      0.561      0.995      0.951\n",
            "                 12080         17         17      0.912          1      0.992      0.959\n",
            "                 12246         10         10      0.873          1      0.995      0.973\n",
            "                 12419          4          4      0.863          1      0.995       0.97\n",
            "                 12777         19         19      0.842          1      0.985      0.953\n",
            "                 13394         17         17       0.92          1      0.992       0.98\n",
            "                 13899         28         28      0.946          1      0.968      0.943\n",
            "                 16231         65         65      0.975          1      0.995      0.995\n",
            "                 16261         72         72      0.992          1      0.995      0.995\n",
            "                 16547         83         83      0.981          1      0.986      0.986\n",
            "                 16550         67         67      0.992          1      0.995      0.988\n",
            "                 16687         14         14      0.687      0.857      0.896      0.868\n",
            "                 18109         16         16      0.964          1      0.995      0.995\n",
            "                 18146         25         25      0.869          1      0.987      0.978\n",
            "                 18356         20         20      0.682          1      0.889      0.889\n",
            "                 19231         24         24      0.821          1      0.981      0.977\n",
            "                 19551         20         20      0.972          1      0.995      0.979\n",
            "                 19606         20         20       0.91          1      0.995      0.995\n",
            "                 19860         38         38      0.982          1      0.995      0.995\n",
            "                 20013         18         18      0.691      0.944      0.919      0.919\n",
            "                 20237         67         67      0.963          1      0.994      0.993\n",
            "                 20876         44         44      0.986          1      0.995      0.995\n",
            "                 21025         17         17      0.967          1      0.995      0.995\n",
            "                 21324         27         27        0.9          1      0.979       0.97\n",
            "                 21770         15         15      0.964          1      0.995      0.995\n",
            "                 22073         29         29      0.954          1      0.994      0.989\n",
            "                 22346         45         45      0.987          1      0.995      0.988\n",
            "                 22361         20         20      0.971          1      0.995      0.995\n",
            "                 22626         17         17      0.966          1      0.995      0.987\n",
            "                 23202          6          6      0.913          1      0.995      0.953\n",
            "                 23222         15         15      0.945          1      0.995      0.986\n",
            "                 24849         13         13      0.951          1      0.995      0.985\n",
            "                 25366         50         50      0.987          1      0.995      0.995\n",
            "                 25437         38         38      0.985          1      0.995      0.995\n",
            "                 25468         41         41      0.986          1      0.995      0.995\n",
            "                 27652         35         35      0.983          1      0.995      0.995\n",
            "                 27732         54         54      0.953      0.981      0.987      0.987\n",
            "                 27776         42         42      0.986          1      0.995      0.995\n",
            "                 27925         23         23      0.973          1      0.995      0.995\n",
            "                 27992          6          6       0.92          1      0.995      0.995\n",
            "                 28762         44         44      0.982          1      0.995      0.993\n",
            "                 29344         21         21      0.972          1      0.995       0.98\n",
            "                 29450         16         16      0.962          1      0.995      0.995\n",
            "                 29666         71         71      0.965          1      0.988      0.985\n",
            "                 29870          3          3      0.894          1      0.995      0.995\n",
            "                 30307         32         32      0.982          1      0.995      0.989\n",
            "                 31704         17         17      0.968          1      0.995      0.995\n",
            "                 31862         28         28      0.916          1      0.987      0.987\n",
            "                 31884         42         42       0.96      0.976      0.992       0.99\n",
            "                 32309         27         27      0.911          1      0.993      0.987\n",
            "                 33008         20         20      0.972          1      0.995      0.995\n",
            "                 33207         15         15      0.957          1      0.995      0.995\n",
            "                 33877          4          4      0.854          1      0.995      0.937\n",
            "                 33879         22         22       0.85          1      0.958      0.953\n",
            "                 34596         41         41      0.966          1      0.995      0.991\n",
            "                 35205         81         81      0.946      0.988      0.985      0.977\n",
            "                 36636         66         66      0.947          1      0.993      0.993\n",
            "                 38161         27         27      0.862          1      0.993      0.973\n",
            "                 41767         28         28      0.939          1      0.992      0.977\n",
            "                 44198         18         18      0.969          1      0.995      0.975\n",
            "Speed: 1.1ms preprocess, 4.0ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 05:09:30] 최종 테스트 수행 중...\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 05:09:30] [Epoch 50/50] Test 시작\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 05:09:31] Epoch [50/50] - Test: 22 images, 86 detections, Avg Conf: 0.9740\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 05:09:33] Submission 생성 시작: /content/drive/MyDrive/oraldrug/4.drug_Augmentation/submission/submission20251215050933\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
            "[2025-12-15 05:09:47] Submission 생성 완료: /content/drive/MyDrive/oraldrug/4.drug_Augmentation/submission/submission20251215050933\n",
            "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Rka7xZLY9nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6ff3e4c"
      },
      "source": [
        "# Task\n",
        "The `BASE_DIR` variable and dependent paths (`LOG_FILE`, `SUBMISSION_DIR`, `MODEL_FILES`, `RESULT_CSV`) are incorrectly set due to an issue in the execution flow. The task is to correct these paths to ensure all output files are saved within the `/content/images` directory as intended by the plan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99af09d9"
      },
      "source": [
        "## Set BASE_DIR\n",
        "\n",
        "### Subtask:\n",
        "BASE_DIR 변수를 /content/images로 설정하여 모든 출력 파일(로그, 모델 등)이 /content/images 디렉토리 내에 저장되도록 수정합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e2c265c"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to set the `BASE_DIR` variable to `/content/images` so that all output files are stored within this directory. I need to modify the relevant code cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d9a8d4f",
        "outputId": "20613091-20df-4318-c8d4-c0d685814294"
      },
      "source": [
        "VER = \"2025.12.12.001.bestOne\"\n",
        "#BASE_DIR = \"/content/drive/MyDrive/codeit/data\"\n",
        "#BASE_DIR = r\"D:\\01.project\\EntryPrj\\data\"\n",
        "BASE_DIR = '/content/images' # 출력 파일(로그, 모델 등)이 저장될 실제 디렉토리 경로\n",
        "\n",
        "print(\"설정된 BASE_DIR:\", BASE_DIR)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "설정된 BASE_DIR: /content/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e1a7515"
      },
      "source": [
        "**Reasoning**:\n",
        "The `BASE_DIR` variable has been correctly set to `/content/images`. Now, I need to re-execute the cells that define other paths (`LOG_FILE`, `SUBMISSTION_DIR`, `MODEL_FILES`, `RESULT_CSV`) to ensure they are updated based on the new `BASE_DIR` value. This will make sure all output files are saved within the `/content/images` directory as intended.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "c1409168",
        "outputId": "8160ec9b-6601-4e6e-930e-bddeb5ca977a"
      },
      "source": [
        "LOG_FILE = os.path.join(BASE_DIR, \"operation.log\")\n",
        "SUBMISSTION_DIR = os.path.join(BASE_DIR, \"submission\")\n",
        "\n",
        "print(f\"설정된 LOG_FILE: {LOG_FILE}\")\n",
        "print(f\"설정된 SUBMISSTION_DIR: {SUBMISSTION_DIR}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20922471.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLOG_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"operation.log\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mSUBMISSTION_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"submission\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"설정된 LOG_FILE: {LOG_FILE}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"설정된 SUBMISSTION_DIR: {SUBMISSTION_DIR}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cc4d1dd"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to update file paths failed because the `os` module was not imported. I need to add `import os` to the cell to make `os.path.join` available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64c0a3ca",
        "outputId": "d3a5e096-916e-4095-f5e6-57c816447b27"
      },
      "source": [
        "import os\n",
        "\n",
        "LOG_FILE = os.path.join(BASE_DIR, \"operation.log\")\n",
        "SUBMISSTION_DIR = os.path.join(BASE_DIR, \"submission\")\n",
        "\n",
        "print(f\"설정된 LOG_FILE: {LOG_FILE}\")\n",
        "print(f\"설정된 SUBMISSTION_DIR: {SUBMISSTION_DIR}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "설정된 LOG_FILE: /content/images/operation.log\n",
            "설정된 SUBMISSTION_DIR: /content/images/submission\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5288d143"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `LOG_FILE` and `SUBMISSTION_DIR` are correctly updated to reflect the new `BASE_DIR`, I need to similarly update `MODEL_FILES` and `RESULT_CSV` to ensure all output files are saved within the `/content/images` directory as specified in the main task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "43f883ea",
        "outputId": "894ffe89-26a6-4c46-e32b-14d90e726dc8"
      },
      "source": [
        "MODEL_FILES = os.path.join(BASE_DIR, \"modelfiles\")\n",
        "RESULT_CSV = os.path.join(BASE_DIR, \"entryprj.csv\")\n",
        "DEVICE_TYPE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"설정된 MODEL_FILES: {MODEL_FILES}\")\n",
        "print(f\"설정된 RESULT_CSV: {RESULT_CSV}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-296830019.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMODEL_FILES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"modelfiles\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mRESULT_CSV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"entryprj.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"설정된 MODEL_FILES: {MODEL_FILES}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "034f8cf5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because the `torch` module was not imported. I need to add `import torch` to ensure `torch.device` is recognized and executed correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e718692",
        "outputId": "09669bfa-df21-4fd3-d2f3-94387e2c8750"
      },
      "source": [
        "import torch\n",
        "\n",
        "MODEL_FILES = os.path.join(BASE_DIR, \"modelfiles\")\n",
        "RESULT_CSV = os.path.join(BASE_DIR, \"entryprj.csv\")\n",
        "DEVICE_TYPE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"설정된 MODEL_FILES: {MODEL_FILES}\")\n",
        "print(f\"설정된 RESULT_CSV: {RESULT_CSV}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "설정된 MODEL_FILES: /content/images/modelfiles\n",
            "설정된 RESULT_CSV: /content/images/entryprj.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f09d7fad"
      },
      "source": [
        "## Review GetConfig Function\n",
        "\n",
        "### Subtask:\n",
        "`GetConfig` 함수가 `data_dir`를 입력받아 `train_images` 및 `test_images` 경로를 정확하게 반환하는지 다시 확인합니다. 현재 `/content/images`가 `data_dir`로 사용될 때, 이 함수는 `/content/images/train_images`와 `/content/images/test_images`를 올바르게 생성합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b3d12d4"
      },
      "source": [
        "**Reasoning**:\n",
        "To verify the `GetConfig` function, I need to call it with the `BASE_DIR` and print the returned `image_dir` and `test_img_dir`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "ccc0d2e1",
        "outputId": "bba670d8-249b-43b4-af42-a859ace26bf2"
      },
      "source": [
        "image_dir, annotation_dir, yaml_file, yaml_label_dir, test_img_dir = GetConfig(BASE_DIR)\n",
        "print(f\"Verified image_dir: {image_dir}\")\n",
        "print(f\"Verified test_img_dir: {test_img_dir}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'GetConfig' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-241800881.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myaml_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myaml_label_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_img_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Verified image_dir: {image_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Verified test_img_dir: {test_img_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GetConfig' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1307deec"
      },
      "source": [
        "**Reasoning**:\n",
        "The `GetConfig` function was not defined in the current execution environment, leading to a `NameError`. I need to re-define the `GetConfig` function and then call it to verify the paths.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b80a192",
        "outputId": "db94deff-ef58-4017-9ff0-60f0a721ba23"
      },
      "source": [
        "def GetConfig(data_dir):\n",
        "    \"\"\"\n",
        "    데이터 디렉토리로부터 필요한 경로들을 생성\n",
        "\n",
        "    Args:\n",
        "        data_dir: 데이터 루트 디렉토리 (예: content\\images\\)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (image_dir, annotation_dir, yaml_file, yaml_label_dir, test_img_dir)\n",
        "    \"\"\"\n",
        "    image_dir = os.path.join(data_dir, \"train_images\")\n",
        "    annotation_dir = os.path.join(data_dir, \"train_annotations\")\n",
        "    yaml_file = os.path.join(data_dir, \"yolo_yaml.yaml\")\n",
        "    yaml_label_dir = os.path.join(data_dir, \"yolo_labels\")\n",
        "    # test_img_dir는 data_dir 내부에 위치하는 것으로 수정\n",
        "    test_img_dir = os.path.join(data_dir, \"test_images\")\n",
        "\n",
        "    print(f\"설정된 image_dir: {image_dir}\")\n",
        "    print(f\"설정된 test_img_dir: {test_img_dir}\")\n",
        "\n",
        "    return image_dir, annotation_dir, yaml_file, yaml_label_dir, test_img_dir\n",
        "\n",
        "image_dir, annotation_dir, yaml_file, yaml_label_dir, test_img_dir = GetConfig(BASE_DIR)\n",
        "print(f\"Verified image_dir: {image_dir}\")\n",
        "print(f\"Verified test_img_dir: {test_img_dir}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "설정된 image_dir: /content/images/train_images\n",
            "설정된 test_img_dir: /content/images/test_images\n",
            "Verified image_dir: /content/images/train_images\n",
            "Verified test_img_dir: /content/images/test_images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:6: SyntaxWarning: invalid escape sequence '\\i'\n",
            "<>:6: SyntaxWarning: invalid escape sequence '\\i'\n",
            "/tmp/ipython-input-4004522930.py:6: SyntaxWarning: invalid escape sequence '\\i'\n",
            "  data_dir: 데이터 루트 디렉토리 (예: content\\images\\)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d15469d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous output showed a `SyntaxWarning` due to an invalid escape sequence in the docstring. I need to correct the docstring to use forward slashes for path examples to remove this warning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c3e6ce9",
        "outputId": "155aaf11-1055-455e-ae0e-b3363018e88a"
      },
      "source": [
        "def GetConfig(data_dir):\n",
        "    \"\"\"\n",
        "    데이터 디렉토리로부터 필요한 경로들을 생성\n",
        "\n",
        "    Args:\n",
        "        data_dir: 데이터 루트 디렉토리 (예: content/images/)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (image_dir, annotation_dir, yaml_file, yaml_label_dir, test_img_dir)\n",
        "    \"\"\"\n",
        "    image_dir = os.path.join(data_dir, \"train_images\")\n",
        "    annotation_dir = os.path.join(data_dir, \"train_annotations\")\n",
        "    yaml_file = os.path.join(data_dir, \"yolo_yaml.yaml\")\n",
        "    yaml_label_dir = os.path.join(data_dir, \"yolo_labels\")\n",
        "    # test_img_dir는 data_dir 내부에 위치하는 것으로 수정\n",
        "    test_img_dir = os.path.join(data_dir, \"test_images\")\n",
        "\n",
        "    print(f\"설정된 image_dir: {image_dir}\")\n",
        "    print(f\"설정된 test_img_dir: {test_img_dir}\")\n",
        "\n",
        "    return image_dir, annotation_dir, yaml_file, yaml_label_dir, test_img_dir\n",
        "\n",
        "image_dir, annotation_dir, yaml_file, yaml_label_dir, test_img_dir = GetConfig(BASE_DIR)\n",
        "print(f\"Verified image_dir: {image_dir}\")\n",
        "print(f\"Verified test_img_dir: {test_img_dir}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "설정된 image_dir: /content/images/train_images\n",
            "설정된 test_img_dir: /content/images/test_images\n",
            "Verified image_dir: /content/images/train_images\n",
            "Verified test_img_dir: /content/images/test_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c9d9704"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "모든 경로 설정이 사용자님의 요청에 맞게 완료되었습니다. 학습을 시작할 준비가 되었습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01ccc2a6"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The `BASE_DIR` variable was successfully updated to `/content/images`, ensuring all output files are directed to the correct location.\n",
        "*   Dependent paths including `LOG_FILE`, `SUBMISSTION_DIR`, `MODEL_FILES`, and `RESULT_CSV` were correctly reconfigured to reside within the `/content/images` directory, for example, `LOG_FILE` is now `/content/images/operation.log` and `SUBMISSTION_DIR` is `/content/images/submission`.\n",
        "*   The `GetConfig` function was verified to correctly generate training and testing image directories. Specifically, when `BASE_DIR` (set to `/content/images`) is passed, it returns `image_dir` as `/content/images/train_images` and `test_img_dir` as `/content/images/test_images`.\n",
        "*   Initial errors, such as `NameError` due to missing `os` and `torch` module imports, and a `SyntaxWarning` within the `GetConfig` function's docstring, were identified and resolved during the process.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The path configuration is now consistent with the requirement that all output files and data directories are stored within the `/content/images` directory, setting up a stable environment for subsequent tasks.\n",
        "*   With the `BASE_DIR` and all dependent paths correctly configured, the system is ready to proceed with data loading, model training, and result saving, utilizing the newly established directory structure.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}